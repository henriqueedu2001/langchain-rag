{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db2b4248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7312ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"sample_paper_raw_text.txt\", encoding=\"utf-8\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aaf34501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='FOUNDATIONAL MODELS IN MEDICAL IMAGING : A\n",
      "COMPREHENSIVE SURVEY AND FUTURE VISION\n",
      "Bobby Azad\n",
      "Electrical Engineering and Computer Science Department\n",
      "South Dakota State University\n",
      "Brookings, USA\n",
      "Reza Azad\n",
      "Faculty of Electrical Engineering and Information Technology\n",
      "RWTH Aachen University\n",
      "Aachen, Germany\n",
      "Sania Eskandari\n",
      "Department of Electrical Engineering\n",
      "University of Kentucky\n",
      "Lexington, USA\n",
      "Afshin Bozorgpour\n",
      "Faculty of Informatics and Data Science\n",
      "University of Regensburg\n",
      "Regensburg, Germany\n",
      "Amirhossein Kazerouni\n",
      "School of Electrical Engineering\n",
      "Iran University of Science and Technology\n",
      "Tehran, Iran\n",
      "Islem Rekik\n",
      "BASIRA Lab, Imperial-X and Computing Department\n",
      "Imperial College London\n",
      "London, UK\n",
      "Dorit Merhof∗\n",
      "Faculty of Informatics and Data Science\n",
      "University of Regensburg\n",
      "Regensburg, Germany\n",
      "ABSTRACT\n",
      "Foundation models, large-scale, pre-trained deep-learning models adapted to a wide range of downstream\n",
      "tasks have gained significant interest lately in various deep-learning problems undergoing a paradigm\n",
      "shift with the rise of these models. Trained on large-scale dataset to bridge the gap between different\n",
      "modalities, foundation models facilitate contextual reasoning, generalization, and prompt capabilities\n",
      "at test time. The predictions of these models can be adjusted for new tasks by augmenting the model\n",
      "input with task-specific hints called prompts without requiring extensive labeled data and retraining.\n",
      "Capitalizing on the advances in computer vision, medical imaging has also marked a growing interest in\n",
      "these models. With the aim of assisting researchers in navigating this direction, this survey intends to\n",
      "provide a comprehensive overview of foundation models in the domain of medical imaging. Specifically,\n",
      "we initiate our exploration by providing an exposition of the fundamental concepts forming the basis\n",
      "of foundation models. Subsequently, we offer a methodical taxonomy of foundation models within the\n",
      "medical domain, proposing a classification system primarily structured around training strategies, while\n",
      "also incorporating additional facets such as application domains, imaging modalities, specific organs of\n",
      "interest, and the algorithms integral to these models. Furthermore, we emphasize the practical use case\n",
      "of some selected approaches and then discuss the opportunities, applications, and future directions of\n",
      "these large-scale pre-trained models, for analyzing medical images. In the same vein, we address the\n",
      "prevailing challenges and research pathways associated with foundational models in medical imaging.\n",
      "These encompass the areas of interpretability, data management, computational requirements, and the\n",
      "nuanced issue of contextual comprehension. Finally, we gather the over-viewed studies with their available\n",
      "open-source implementations at our GitHub. We aim to update the relevant latest papers within it regularly.\n",
      "Keywords Foundation models · Deep learning · Language and vision · Large language models · Score-based models ·\n",
      "Self-supervised learning · Medical applications · Survey\n",
      "∗Corresponding author: Dorit Merhof, dorit.merhofur.de\n",
      "arXiv:2310.18689v1  [cs.CV]  28 Oct 2023Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision\n",
      "1 Introduction\n",
      "Medical imaging is at the forefront of healthcare, playing a pivotal role in diagnosing and treating diseases. Recent\n",
      "advancements in Artificial Intelligence (AI) have given rise to a new era in the field of medical imaging, driven by the\n",
      "development of Foundation Models (FMs). Foundation Models (FMs) are a type of artificial intelligence (AI) model that\n",
      "exhibit significant progress in their development. These models are typically trained on extensive, diverse dataset, frequently\n",
      "utilizing self-supervision techniques on a massive scale. Following this initial training, they can be further adapted, such as\n",
      "through fine-tuning, for a wide array of downstream tasks that are related to the original training data [1].\n",
      "In contrast to the conventional deep learning paradigm, which heavily relies on large-scale, task-specific, and crowd-labeled\n",
      "data to train individual deep neural networks (DNNs) for various visual recognition tasks, FMs provide a more efficient\n",
      "alternative. They are pretrained on large-scale dataset that are nearly unlimited in availability, enabling straightforward\n",
      "application to downstream tasks with only a limited amount of labeled data. This shift in approach shows potential for\n",
      "significantly decreasing the labor and time usually necessary for such tasks. The recent surge can be attributed to the\n",
      "progress made possible by large language models (LLMs) and the expansion of data and size [2]. Models such as GPT-3\n",
      "[3], PaLM [4], Galactica [5], and LLaMA [ 6] have exhibited strong ability to comprehend natural language and solve\n",
      "complex tasks with zero/few-shot learning, attaining remarkable results without requiring extensive task-specific data.\n",
      "Large-scale vision foundation models are currently making significant advances in perception tasks, as highlighted by [7, 8].\n",
      "Specifically, vision-language models (VLM) are pre-trained with large-scale image-text pairs and are then directly applicable\n",
      "to downstream visual recognition tasks. VLMs generally consist of three fundamental parts: textual features, visual features,\n",
      "and a fusion module. These elements work together in harmony, allowing the models to efficiently use text and visual\n",
      "data to generate contextually appropriate and logical results. Specifically, the pre-training of VLMs typically adheres to\n",
      "vision-language objectives that aid in acquiring image-text correlations from large collections of image-text pairs. For\n",
      "instance, the pioneer study CLIP [9], an image-text matching model, utilizes contrastive learning methods to generate fused\n",
      "representations for images and texts. The learning objective is to minimize the gap between the representation of an image\n",
      "and its corresponding text, while simultaneously increasing the separation between the representations of unrelated pairs.\n",
      "In addition to this so-called “Textually Prompted Models (TPMs)\", researchers have also explored Feature Maps (FMs)\n",
      "that can be prompted by visual inputs (points, boxes, masks) which we refer to as “Visually Prompted Models (VPMs)\"\n",
      "[2] (see fig. 2 for a visual depiction of both). Recently, the Segment Anything (SA) model [ 10] has garnered significant\n",
      "attention in the vision community. SAM is a promptable model developed for the purpose of broad image segmentation.\n",
      "It was trained using a promptable segmentation task that enables powerful zero-shot generalization on 11 million images\n",
      "and over 1 billion masks. Furthermore, SAM has been expanded and refined through training on a large dataset, which\n",
      "encompasses 4.6 million medical images and 19.7 million corresponding masks [11]. This dataset offers a rich diversity,\n",
      "covering 10 distinct medical data modalities, and featuring annotations for 4 anatomical structures in addition to lesions.\n",
      "The training regimen is comprehensive, representing 31 major human organs. Notably, it has yielded impressive results that\n",
      "have bolstered the model’s capacity for enhanced generalization.\n",
      "Moreover, this generic visual prompt-based segmentation model has recently been adapted to a wide range of downstream\n",
      "tasks, including medical image analysis [ 12, 13], image inpainting [ 14], style transfer [ 15], and image captioning [ 16]\n",
      "to name a few. Apart from foundational models that rely on textual and visual prompts, research endeavors have also\n",
      "delved into creating models that harmonize various types of paired modalities (such as image-text, and video-audio) to learn\n",
      "representations assisting diverse downstream tasks.\n",
      "The creationn of foundation models has garnered significant attention in the medical AI system development realm\n",
      "[17, 18, 19, 20, 21]. Despite the substantial advancements in biomedical AI, the main methodologies used still tend to be\n",
      "tas-specific models. However, medical practice encompasses various data modalities comprising text, imaging, genomics,\n",
      "and others, making it essentially multimodal [ 22]. Inherently, a Medical Foundation Model (MFM) has the ability to\n",
      "adaptively interpret various medical modalities, including diverse data sources such as images, electronic medical records,\n",
      "lab findings, genomic information, medical diagrams, and textual data [23]. Hence, foundational models have the potential\n",
      "to provide an enhanced foundation for addressing clinical issues, advancing the field of medical imaging, and improving the\n",
      "efficiency and effectiveness of diagnosing and treating diseases, leading to the opportunity to develop a unified biomedical\n",
      "AI system that can interpret complex multimodal data. Due to the acceleration of both biomedical data production and\n",
      "advancements, the influence of these models is expected to expand due to an influx of contributions. As shown in Figure 1,\n",
      "a significant body of research has been devoted to the application of FMs in diverse medical imaging contexts until the\n",
      "first release of our survey in October 2023. These contributions encompass a wide range of potential applications, from\n",
      "fundamental biomedical discoveries to the upgrading of healthcare delivery. Hence, it is advantageous for the community\n",
      "and timely to review the existing literature.\n",
      "This paper provides a holistic overview of the foundation models developed for medical imaging applications. We distinguish\n",
      "existing works inspired by the taxonomy proposed in [2] and highlight the major strengths and shortcomings of the existing\n",
      "methods. We hope that this work will point the way forward, provide a roadmap for researchers, stimulate further interest\n",
      "2Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision\n",
      "/uni00000026/uni00000052/uni00000051/uni00000057/uni00000055/uni00000044/uni00000056/uni00000057/uni0000004c/uni00000059/uni00000048\n",
      "/uni00000026/uni00000052/uni00000051/uni00000059/uni00000048/uni00000055/uni00000056/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000044/uni0000004f\n",
      "/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048\n",
      "/uni0000002b/uni0000005c/uni00000045/uni00000055/uni0000004c/uni00000047\n",
      "/uni00000024/uni00000047/uni00000044/uni00000053/uni00000057/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056\n",
      "/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni0000004f/uni0000004c/uni00000056/uni00000057\n",
      "/uni00000037/uni00000033/uni00000030\n",
      "/uni00000039/uni00000033/uni00000030\n",
      "(a) Algorithms\n",
      "/uni00000026/uni00000037 /uni00000030/uni00000035/uni0000002c\n",
      "/uni0000003b/uni00000010/uni00000055/uni00000044/uni0000005c/uni00000056\n",
      "/uni00000030/uni00000058/uni0000004f/uni00000057/uni0000004c\n",
      "/uni00000029/uni00000058/uni00000051/uni00000047/uni00000058/uni00000056\n",
      "/uni00000030/uni00000058/uni0000004f/uni00000057/uni0000004c\n",
      "/uni00000015/uni00000027/uni00000030/uni00000058/uni0000004f/uni00000057/uni0000004c/uni00000003/uni00000027/uni0000004c/uni00000050/uni00000056/uni00000011\n",
      "/uni0000002b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000053/uni00000044/uni00000057/uni0000004b/uni00000052/uni0000004f/uni00000052/uni0000004a/uni0000005c\n",
      "/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000003/uni00000052/uni00000051/uni0000004f/uni0000005c\n",
      "/uni00000035/uni00000044/uni00000047/uni0000004c/uni00000052/uni0000004f/uni00000052/uni0000004a/uni0000005c\n",
      "/uni0000004c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000056\n",
      "/uni00000035/uni00000048/uni00000057/uni0000004c/uni00000051/uni00000044/uni0000004f\n",
      "/uni0000004c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000056\n",
      "/uni00000030/uni00000058/uni0000004f/uni00000057/uni0000004c/uni00000010/uni00000050/uni00000052/uni00000047/uni00000044/uni0000004f\n",
      "/uni00000032/uni00000057/uni0000004b/uni00000048/uni00000055 (b) Modalities\n",
      "/uni00000025/uni00000055/uni00000044/uni0000004c/uni00000051\n",
      "/uni00000026/uni0000004b/uni00000048/uni00000056/uni00000057\n",
      "/uni00000028/uni0000005c/uni00000048\n",
      "/uni0000002b/uni00000048/uni00000044/uni00000055/uni00000057\n",
      "/uni00000030/uni00000058/uni0000004f/uni00000057/uni0000004c/uni00000010/uni00000052/uni00000055/uni0000004a/uni00000044/uni00000051\n",
      "/uni00000038/uni00000051/uni0000004c/uni00000059/uni00000048/uni00000055/uni00000056/uni00000044/uni0000004f (c) Organs\n",
      "Figure 1: The diagram (a) displays the distribution of published papers categorized by their algorithm, (b) categorizes them\n",
      "by their imaging modalities, and (c) classifies them by the type of organ concerned. It is worth noting that the total number\n",
      "of papers included in the analysis is 40.\n",
      "and enthusiasm within the vision community, and harness the potential of foundation models in the medical discipline. This\n",
      "survey will be regularly updated to reflect the dynamic progress of the MFMs, as this is a rapidly evolving and promising\n",
      "field towards AGI in the biomedical field. Our major contributions include:\n",
      "• We conduct a thorough and exhaustive examination of foundation models proposed in the field of medical imaging,\n",
      "beginning from background and preliminaries for foundation models, to specific applications along with the organ concerned\n",
      "and imaging modality in a hierarchical and structured manner\n",
      "• Our work provides a taxonomized (Figure 3), in-depth analysis (e.g. task/organ-specific research progress and limitations),\n",
      "as well as a discussion of various aspects.\n",
      "• Furthermore, we discuss the challenges and unresolved aspects linked to foundation models in medical imaging. We\n",
      "pinpoint new trends, raise important questions, and propose future directions for further exploration.\n",
      "Output \n",
      "T extual \n",
      "Pr ompts \n",
      "V isual \n",
      "Pr ompts \n",
      "Foundation \n",
      "Model \n",
      "Traditional Models\n",
      "Textual Prompted\n",
      "Models\n",
      "Visual Prompted\n",
      "Models\n",
      "Image \n",
      "Book \n",
      "Article \n",
      "Report \n",
      "QA\n",
      "...\n",
      "Bounding box \n",
      "Points \n",
      "Arrow \n",
      "...\n",
      "Figure 2: Visual illustration of how our extensive classification categorizes existing works into textually and visually\n",
      "prompted models, distinct from traditional vision models.\n",
      "1.1 Clinical Importance\n",
      "In medical imaging, foundation models are reshaping the way research methods are designed and paradigms are approached,\n",
      "paving the way for innovative advancements and pioneering breakthroughs across various sectors owing to some of their\n",
      "inherent properties aligned with the medical domain as follows.\n",
      "Multi-Modality: Despite advances in biomedical AI, most models today are limited to single-task, unimodal functions.\n",
      "For instance, a mammogram interpretation AI excels at breast cancer screening but can’t incorporate patient records, and\n",
      "additional data like MRI, or engage in meaningful dialogue, limiting its real-world applicability.\n",
      "Explainability and Generalization:The absence of explainability in deep learning models can erode trust among clinicians\n",
      "3Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision\n",
      "  F M M I   T a x o n o m y \n",
      "  T P M   ‑   G e ne r a t i v e \n",
      "  2 1 .   C l i n i c a l ‑ B E R T   O r g a n ꞉   C h e s t \n",
      "  M o d a l i t y ꞉   X ‑ r a y \n",
      "  2 2 .   M e d ‑ P a L M   2   O r g a n ꞉   U n i v e r s a l \n",
      "  M o d a l i t y ꞉   T e x t   o n l y \n",
      "  2 3 .   M e d ‑ F l a m i n g o   O r g a n ꞉   U n i v e r s a l \n",
      "  M o d a l i t y ꞉   M u l t i ‑ m o d a l i t y   ( 2 D ) \n",
      "  V P M   ‑   A da pt a t i o n s \n",
      "  2 4 .   M e d S A M   O r g a n ꞉   U n i v e r s a l \n",
      "  M o d a l i t y ꞉   M u l t i ‑ m o d a l i t y   ( 2 D ) \n",
      "  2 5 .   S A M ‑ U   O r g a n ꞉   E y e   ( R e t i n a ) \n",
      "  M o d a l i t y ꞉   F u n d u s \n",
      "  2 6 .   S A M ‑ M e d 2 D   O r g a n ꞉   U n i v e r s a l \n",
      "  M o d a l i t y ꞉   M u l t i ‑ m o d a l i t y   ( 2 D ) \n",
      "  2 7 .   L V M ‑ M e d   O r g a n ꞉   U n i v e r s a l \n",
      "  M o d a l i t y ꞉   M u l t i ‑ m o d a l i t y   ( 2 D ) \n",
      "  2 8 .   A u t o S A M   O r g a n ꞉   H e a r t \n",
      "  M o d a l i t y ꞉   M R I   ( 2 D ) \n",
      "  2 9 .   M S A   O r g a n ꞉   U n i v e r s a l \n",
      "  M o d a l i t y ꞉   M u l t i ‑ m o d a l i t y \n",
      "  3 0 .   S A M e d   O r g a n ꞉   M u t i ‑ o r g a n s \n",
      "  M o d a l i t y ꞉   C T   ( 2 D ) \n",
      "  3 1 .   V i r c h o w   O r g a n ꞉   U n i v e r s a l \n",
      "  M o d a l i t y ꞉   H i s t o p a t h o l o g y \n",
      "  T P M   ‑   C o nv e r s a t i o n a l \n",
      "  3 2 .   P M C ‑ L L a M A   O r g a n ꞉   U n i v e r s a l \n",
      "  M o d a l i t y ꞉   T e x t   o n l y \n",
      "  3 3 .   C l i n i c a l G P T   O r g a n ꞉   U n i v e r s a l \n",
      "  M o d a l i t y ꞉   T e x t   o n l y \n",
      "  3 4 .   L L a V A ‑ M e d   O r g a n ꞉   U n i v e r s a l \n",
      "  M o d a l i t y ꞉   M u l t i ‑ m o d a l i t y   ( 2 D ) \n",
      "  3 5 .   X r a y G P T   O r g a n ꞉   C h e s t \n",
      "  M o d a l i t y ꞉   X ‑ r a y \n",
      "  3 6 .   R a d i o l o g y ‑ L l a m a 2   O r g a n ꞉   C h e s t \n",
      "  M o d a l i t y ꞉   T e x t   o n l y \n",
      "  3 7 .   C h a t C A D   O r g a n ꞉   C h e s t \n",
      "  M o d a l i t y ꞉   X ‑ r a y \n",
      "  3 8 .   D e I D ‑ G P T   O r g a n ꞉   U n i v e r s a l \n",
      "  M o d a l i t y ꞉   T e x t   o n l y \n",
      "  3 9 .   C h a t D o c t o r   O r g a n ꞉   U n i v e r s a l \n",
      "  M o d a l i t y ꞉   T e x t   o n l y \n",
      "  4 0 .   V i s u a l   M e d ‑ A l p a c a   O r g a n ꞉   U n i v e r s a l \n",
      "  M o d a l i t y ꞉   M u l t i ‑ m o d a l i t y   ( 2 D ) \n",
      "  T P M   ‑   C o nt r a s t i v e \n",
      "  9 .   C h e X z e r o   O r g a n ꞉   C h e s t \n",
      "  M o d a l i t y ꞉   X ‑ r a y \n",
      "  1 0 .   M e d C L I P   O r g a n ꞉   C h e s t \n",
      "  M o d a l i t y ꞉   X ‑ r a y \n",
      "  1 1 .   B i o V i L ‑ T   O r g a n ꞉   C h e s t \n",
      "  M o d a l i t y ꞉   X ‑ r a y \n",
      "  1 2 .   C L I P D M ‑ O T S   O r g a n ꞉   M u l t i ‑ o r g a n s \n",
      "  M o d a l i t y ꞉   C T   ( 3 D ) \n",
      "  1 3 .   B i o m e d C L I P   O r g a n ꞉   U n i v e r s a l \n",
      "  M o d a l i t y ꞉   M u l t i ‑ m o d a l i t y   ( 2 D ) \n",
      "  1 4 ,   P T U n i f i e r   O r g a n ꞉   M u l t i ‑ o r g a n s \n",
      "  M o d a l i t y ꞉   R a d i o l o g y   I m a g e s \n",
      "  1 5 .   M I ‑ Z e r o   O r g a n ꞉   U n i v e r s a l \n",
      "  M o d a l i t y ꞉   H i s t o p a t h o l o g y \n",
      "  1 6 .   K o B o   O r g a n ꞉   C h e s t \n",
      "  M o d a l i t y ꞉   X ‑ r a y \n",
      "  1 7 .   W e b P L I P   O r g a n ꞉   U n i v e r s a l \n",
      "  M o d a l i t y ꞉   H i s t o p a t h o l o g y \n",
      "  1 8 .   E L I X R   O r g a n ꞉   C h e s t \n",
      "  M o d a l i t y ꞉   X ‑ r a y \n",
      "  1 9 .   M a C o   O r g a n ꞉   C h e s t \n",
      "  M o d a l i t y ꞉   X ‑ r a y \n",
      "  2 0 .   C I T E   O r g a n ꞉   U n i v e r s a l \n",
      "  M o d a l i t y ꞉   H i s t o p a t h o l o g y \n",
      "  T P M   ‑   H y br i d \n",
      "  7 .   M e d B L I P   O r g a n ꞉   B r a i n \n",
      "  M o d a l i t y ꞉   M R I   ( 3 D ) \n",
      "  8 .   V L M   f o r   V Q A   i n   M I   O r g a n ꞉   U n i v e r s a l \n",
      "  M o d a l i t y ꞉   M u l t i ‑ m o d a l i t y   ( 2 D ) \n",
      "  V P M   ‑   G e ne r a l i s t \n",
      "  1 .   S A M ‑ B ‑ Z S S   O r g a n ꞉   U n i v e r s a l \n",
      "  M o d a l i t y ꞉   M u l t i ‑ m o d a l i t y   ( 2 D ) \n",
      "  2 .   F M   f o r   G M A I   O r g a n ꞉   U n i v e r s a l \n",
      "  M o d a l i t y ꞉   M u l t i ‑ m o d a l i t y   &     M u l t i ‑ m e d i a \n",
      "  3 .   B i o m e d G P T   O r g a n ꞉   U n i v e r s a l \n",
      "  M o d a l i t y ꞉   M u l t i ‑ m o d a l i t y   ( 2 D ) \n",
      "  4 .   M e d ‑ P a l M   M   O r g a n ꞉   U n i v e r s a l \n",
      "  M o d a l i t y ꞉   R a d i o l o g y   i m a g e s \n",
      "  5 .   R a d F M   O r g a n ꞉   M u l t i ‑ o r g a n s \n",
      "  M o d a l i t y ꞉   R a d i o l o g y   i m a g e s \n",
      "  6 .   R E T F o u n d   O r g a n ꞉   E y e   ( R e t i n a ) \n",
      "  M o d a l i t y ꞉   F u n d u s   a n d   O C T \n",
      "Figure 3: The suggested taxonomy for foundational models used in medical imaging research consists of six distinct\n",
      "groups: I) VPM-Generalist, II) TPM-Hybrid, III) TPM-Contrastive, IV) TPM-Generative, V) VPM-Adaptations, and VI)\n",
      "TPM-Conversational. To maintain conciseness, we assign ascending prefix numbers to each category in the paper’s name\n",
      "and cite each study accordingly as follows: 1. [ 24], 2. [23], 3. [25], 4. [22], 5. [26], 6. [27], 7. [28], 8. [29], 9. [30], 10.\n",
      "[31], 11. [32], 12. [33], 13. [34], 14, [35], 15. [36], 16. [37], 17. [38], 18. [39], 19. [40], 20. [18], 21. [41], 22. [42], 23.\n",
      "[43], 24. [12], 25. [44], 26. [11], 27. [21], 28. [45], 29. [46], 30. [47], 31. [48], 32. [49], 33. [50], 34. [51], 35. [52], 36.\n",
      "[53], 37. [54], 38. [55], 39. [56], 40. [57]\n",
      "accustomed to clear clinical insights [58]. The ability of models to generalize across different medical settings is vital due to\n",
      "varying data sources. Foundation models address these issues by offering a unified framework for tasks like detection and\n",
      "classification, often trained on diverse datasets from various medical centers, enhancing their potential for clinical use by\n",
      "ensuring interpretability and broad applicability.\n",
      "Privacy Preservation:The computer vision community has a history of open-sourcing datasets, but in medical imaging,\n",
      "privacy regulations limit data sharing. Foundation models offer a privacy-preserving alternative by allowing knowledge\n",
      "transfer without direct access to sensitive data. Additionally, federated learning enables model training on distributed data\n",
      "while keeping it on local machines, ensuring data privacy. Moreover, foundation models facilitate privacy preservation by\n",
      "generating synthetic data resembling real medical images, eliminating the need for actual patient data in model training.\n",
      "Adaptability: Existing medical AI models struggle when faced with distribution shifts caused by changes in technology,\n",
      "4Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision\n",
      "procedures, settings, or populations. In contrast, MFMs can effectively adapt to these shifts through in-context learning.\n",
      "For instance, a hospital can teach an MFM model to interpret X-rays from a new scanner by providing a few examples as\n",
      "prompts, enabling it to adjust to new data distributions in real time. This capability is mainly seen in large language models\n",
      "and is not common in conventional medical AI models, which would typically require complete retraining with new datasets.\n",
      "Domain Knowledge: Unlike clinicians, traditional medical AI models often lack initial medical domain knowledge,\n",
      "relying solely on statistical associations. Medical imaging foundation models like GMAI can address this limitation by\n",
      "integrating formal medical knowledge, using structures like knowledge graphs and retrieving relevant context from existing\n",
      "databases, improving their performance on specific tasks. In summary, foundation models play a crucial role in advancing\n",
      "medical applications by providing a robust and adaptable framework that enhances efficiency, generalizability, and privacy\n",
      "preservation. Their ability to support various clinical tasks and promote collaboration makes them invaluable tools for\n",
      "improving patient care and medical research.\n",
      "1.2 Relevant Surveys\n",
      "With the recent success of foundation models, there has been a surge of surveys and contributions in this domain. Some\n",
      "of the reviews investigate recent advances in LLMs, distinguishing different aspects of LLMs by analyzing the impact of\n",
      "pre-training adaptation tuning, utilization, and evaluation [ 59, 60, 61]. In the context of vision models, the work of [ 2]\n",
      "provides a comprehensive review of FMs including their typical architecture design, training objectives, and prompting\n",
      "mechanisms. The work of [ 62] delivers a comprehensive survey of research in prompt engineering on diverse types of\n",
      "vision-language models, organizing existing prompt-engineering approaches from a new perspective. Besides, [63] provides\n",
      "a systematic review of visual language models for various visual recognition tasks including image classification, object\n",
      "detection, and semantic segmentation. In the medical imaging field, [23] identifies the potential applications, opportunities,\n",
      "and challenges of MFMs. The work of [24] provides a comprehensive and objective evaluation of SAM on medical image\n",
      "segmentation, while [64] discusses the spectrum, and future directions of foundation models. However, different from the\n",
      "aforementioned works, we devise a multi-perspective taxonomy of foundation models in the medical community, providing\n",
      "a systematical category of research in medical foundation models and their applications dividing them into textually\n",
      "prompted models, and visually prompted models where each paper is broadly classified according to the proposed algorithm\n",
      "along with the organ concerned and imaging modality, respectively. We present the concepts and theoretical foundations\n",
      "behind foundation models ranging from training objectives and instruction-aligning to prompt engineering (Section 2). In\n",
      "Section 3.1, we comprehensively cover an extensive and up-to-date overview of the recent medical foundation models, as\n",
      "shown in Figure 3. We wrap up this survey by pinpointing future directions and open challenges facing foundation models\n",
      "in medical imaging in Section 5.\n",
      "1.3 Search Strategy\n",
      "We conducted extensive searches across various platforms, such as DBLP, Google Scholar, and Arxiv Sanity Preserver. We\n",
      "leveraged their search capabilities to create tailored queries and compile comprehensive lists of academic works. These\n",
      "searches encompassed a broad spectrum of scholarly publications, including peer-reviewed journal articles, conference\n",
      "papers, workshop materials, non-peer-reviewed content, and preprints. We tailored our search criteria to achieve this\n",
      "diversity. Our specific search queries consisted of keywords (foundation* | generalist* | medical* | {Task}*),\n",
      "(med-{FM} | foundation*), (foundation* | biomedical* | image* | model*), where {FM} and {Task} refer\n",
      "to one well-known vision foundation model(such as PaLM, CLIP, etc) or Tasks (such as Segmentation, Question Answering,\n",
      "etc) in medical imaging. We then applied filtering to eliminate false positives, ensuring that only papers related to foundation\n",
      "models were included in our analysis.\n",
      "1.4 Paper Organization.\n",
      "The rest of the survey is organized as follows. Section 2 presents the background and preliminaries for foundation models.\n",
      "We adopt the taxonomy of [2] and categorize previous studies into two main groups: those prompted by textual inputs\n",
      "(discussed in section 3.1) and those driven by visual cues (discussed in section 3.2). In the context of textually prompted\n",
      "foundation models, we further subdivide them into contrastive, generative, hybrid (combining contrastive and generative\n",
      "approaches), and conversational visual language models. In addition, we differentiate textually prompted models into\n",
      "adaptations and generalist models. Furthermore, Section 5 reveals the risk, open problems, and future directions of\n",
      "foundation models. Finally, we conclude our research in Section 6.\n",
      "5Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision\n",
      "2 Preliminaries\n",
      "The term \"foundational models\" made its debut at Stanford Institute for Human-Centred AI in [1] with the definition of \"the\n",
      "base models trained on large-scale data in a self-supervised or semi-supervised manner that can be adapted for several\n",
      "other downstream tasks\". Specifically, inspired by the surge of large language models (LLMs), using the basic fundamentals\n",
      "of deep learning such as DNNs and self-supervised learning, foundation models have emerged by massively scaling up both\n",
      "data and model size. In this section, we introduce the basic model architectures, concepts, and settings behind FMs focusing\n",
      "on contributing factors for these models in computer vision such as training objectives, instruction-aligning, inference\n",
      "procedure and prompting.\n",
      "2.1 Pre-training Objectives\n",
      "Diverse pretraining objectives have been devised to learn a rich understanding of the relationship between vision and\n",
      "language [65, 66, 67, 68]. We broadly categorize them into contrastive and generative objectives.\n",
      "2.1.1 Contrastive Objectives\n",
      "Contrastive objectives instruct models to acquire distinctive representations [ 69, 67] by bringing related sample pairs\n",
      "closer together while pushing unrelated pairs farther apart within the feature space. Specifically, Image Contrastive Loss\n",
      "(ICL) aims to learn discriminative image features making a query image closely resemble its positive keys (i.e., its data\n",
      "augmentations) while ensuring it remains distant from its negative keys (i.e., other images) within the embedding space.\n",
      "Consider a batch of B images, contrastive objectives such as InfoNCE [ 70] and its variations [ 67, 69], LInfoNCE\n",
      "I can be\n",
      "expressed as:\n",
      "LInfoNCE\n",
      "I = − 1\n",
      "B\n",
      "BX\n",
      "i=1\n",
      "log\n",
      "exp\n",
      "\u0010\n",
      "θquery\n",
      "i · θpositive\n",
      "+ /τ\n",
      "\u0011\n",
      "PB+1\n",
      "j=1,j̸=i exp\n",
      "\u0010\n",
      "θquery\n",
      "i · θkey\n",
      "j /τ\n",
      "\u0011\n",
      "where θquery\n",
      "i represents the query embedding, {θkey\n",
      "j }B+1\n",
      "j=1,j̸=i are the key embeddings, where θpositive\n",
      "+ denotes the positive\n",
      "key corresponding to θquery\n",
      "i , while the rest are considered negative keys. The hyperparameter τ governs the density of the\n",
      "learned representation.\n",
      "Image-Text Contrastive Loss (ITCL)Seeks to develop distinctive image-text representations by bringing together the\n",
      "embeddings of matched images and texts and pushing apart those that do not match [68, 7]. Let (i, ti) represent the i-th\n",
      "image-text example, then, the image-to-text loss is calculated as:\n",
      "LI→T = −log\n",
      "\"\n",
      "exp (θi · θ+/τ)PN\n",
      "j=1 exp (θi · θj/τ)\n",
      "#\n",
      "where N is the total number of such pairs, and θi corresponds to the embedding for image i, while θ+ and θj denote positive\n",
      "and negative text representations, respectively. The losses are computed with a focus on the relationship between images\n",
      "and texts while considering the temperature parameter τ.\n",
      "The text-to-image loss is also calculated similarly, and the total loss is the sum of these two terms:\n",
      "LITC = 1\n",
      "N\n",
      "NX\n",
      "i=1\n",
      "[LI→T + LT→I]\n",
      "Akin to ICL and ITCL, various contrastive loss functions have also found an application (SimCLR [69, 71], FILIP Loss\n",
      "[72], Region-Word Alignment (RWA)[73], and Region-Word Contrastive (RWC)[74]).\n",
      "2.1.2 Generative Objectives\n",
      "Generative objectives involve teaching networks to produce image or text data, which allows them to acquire semantic\n",
      "features, accomplished through tasks like image generation [75], and language generation [76].\n",
      "Masked Image Modelling (MIM)involves the acquisition of cross-patch correlations by applying masking and image\n",
      "reconstruction techniques. In MIM, a selection of patches within an input image is randomly masked, and the encoder is\n",
      "6Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision\n",
      "trained to reconstruct these masked patches based on the unmasked patches. For a given batch ofB images, the loss function\n",
      "is formulated as:\n",
      "LMIM = − 1\n",
      "B\n",
      "BX\n",
      "i=1\n",
      "log fθ\n",
      "\u0000\n",
      "¯xI\n",
      "i | ˆxI\n",
      "i\n",
      "\u0001\n",
      ",\n",
      "where ¯xI\n",
      "i and ˆxI\n",
      "i represent the masked and unmasked patches within xI\n",
      "i , respectively [63].\n",
      "Masked Language Modelling (MLM)is a widely adopted pretraining objective in Natural Language Processing (NLP). In\n",
      "MLM, a specific percentage of input text tokens is randomly masked, and these masked tokens are reconstructed using the\n",
      "unmasked ones. The loss function for MLM can be expressed as:\n",
      "LMLM = − 1\n",
      "B\n",
      "BX\n",
      "i=1\n",
      "log fθ\n",
      "\u0000\n",
      "¯xT\n",
      "i | ˆxT\n",
      "i\n",
      "\u0001\n",
      ",\n",
      "where ¯xT\n",
      "i and ˆxT\n",
      "i denote the masked and unmasked tokens within xT\n",
      "i , respectively, and B denotes the batch size [63].\n",
      "Likewise, diverse additional generative loss functions have been introduced in the field including Masked Multimodal\n",
      "Modeling (MMM) loss [ 77], Image-conditioned Masked Language Modeling (IMLM) loss [ 78], and Captioning with\n",
      "Parallel Prediction (CapPa) [79].\n",
      "2.2 Pre-training Tasks\n",
      "As discussed in section 2.1, FMs pre-training has been studied with typical approaches including contrastive objectives,\n",
      "and generative objectives. In natural language processing, certain pre-training tasks include masked language modeling,\n",
      "where words in the input sequence are randomly hidden, and the model predicts these hidden words during pre-training.\n",
      "Another task involves next-sentence-prediction, where pairs of sentences from distinct documents are presented, and the\n",
      "model determines whether the order of these sentences is accurate. Additionally, there’s the denoising auto-encoder task,\n",
      "which introduces noise into the original text corpus and then aims to reconstruct the pristine input using the noisy version\n",
      "of the corpus. Likewise, to enable the generalization of learned representations to a range of downstream vision domains,\n",
      "pretext tasks such as inpainting [80], auxiliary supervised discriminative tasks, and data reconstruction tasks [81] are used in\n",
      "the pre-training stage.\n",
      "2.3 Instruction-Aligning\n",
      "Instruction-aligning methods aim to let the LM follow human intents and generate meaningful outputs. This process\n",
      "involves either fine-tuning the model on a diverse set of tasks with human-annotated prompts and feedback (RLHF) [82],\n",
      "conducting supervised fine-tuning on publicly available benchmarks and datasets, which are augmented with manually\n",
      "or automatically generated instructions, and improving the reasoning ability of LLMs by instructing them to produce a\n",
      "sequence of intermediate actions that ultimately lead to the solution of a multi-step problem (Chain-of-thought) [83].\n",
      "2.4 Prompt Engineering\n",
      "Prompt engineering refers to a method that enhances a large pre-trained model by incorporating task-specific hints, referred\n",
      "to as prompts, to tailor the model for new tasks enabling the power to acquire predictions based only on prompts without\n",
      "updating model parameters [62]. In the context of large language models (LLMs) prompting techniques can be categorized\n",
      "into two primary groups depending on the clarity of the templates they employ: \"soft prompts\" (optimizable, learnable)\n",
      "and \"hard prompts (manually crafted text prompts)\". Within the \"hard prompt\" category, there are four subcategories: task\n",
      "instructions, in-context learning, retrieval-based prompting, and chain-of-thought prompting. In contrast, \"soft prompts\"\n",
      "fall into two strategies: prompt tuning and prefix token tuning, which differ in whether they introduce new tokens into the\n",
      "model’s architecture or simply attach them to the input. In the vision domain, prompt engineering facilitates the acquisition\n",
      "of joint multi-modal representations (e.g., CLIP [68] for image classification of ALIGN [84]) to introduce human interaction\n",
      "to the foundational models and employs vision-language models for visual tasks.\n",
      "3 Foundational Models for Medical Imaging\n",
      "Establishing a taxonomy for foundational models in medical imaging analysis follows the standard practices commonly\n",
      "employed in the field. However, we distinguish our approach by providing extensive additional information for each\n",
      "7Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision\n",
      "sub-category as presented in Figure 3. In this section, we explore foundational-based methods, which have been introduced\n",
      "to tackle diverse challenges in medical imaging analysis through the design of distinct training strategies.\n",
      "3.1 Textually Prompted Models\n",
      "3.1.1 Contrastive\n",
      "Contrastive textually prompted models are increasingly recognized in the foundational models for medical imaging. They\n",
      "learn representations that encapsulate the semantics and relationships between medical images and their textual prompts.\n",
      "Leveraging contrastive learning objectives, they draw similar image-text pairs closer in the feature space while pushing\n",
      "dissimilar pairs apart. These models are pivotal for image classification, segmentation, and retrieval tasks. Architectural\n",
      "explorations have ranged from dual-encoder designs—with separate visual and language encoders—to fusion designs\n",
      "that merge image and text representations via decoder and transformer-based architectures. Their potential in medical\n",
      "imaging tasks such as lesion detection, disease classification, and image synthesis is evident in numerous studies. In this\n",
      "direction, Wang et al. [31] introduced the MedCLIP framework, demonstrating its superiority over state-of-the-art methods\n",
      "in zero-shot prediction, supervised classification, and image-text retrieval. Expanding upon the success of models like CLIP,\n",
      "[34] unveiled BiomedCLIP, tailored for biomedical vision-language processing. Its training on a vast dataset of 15 million\n",
      "figure-caption pairs highlighted the efficacy of specialized pretraining in the medical imaging field.\n",
      "Visual language pre-training has made significant advancements in representation learning, especially evident in challenging\n",
      "scenarios like zero-shot transfer tasks in open-set image recognition. Nevertheless, computational pathology hasn’t delved\n",
      "deeply into zero-shot transfer due to data scarcity and challenges presented by gigapixel histopathology whole-slide images\n",
      "(WSI). Drawing inspiration from the success of multiple-instance learning in weakly supervised learning tasks, [ 36]\n",
      "introduced MI-Zero (Figure 4). In this method, each WSI is divided into smaller tiles, referred to as instances, which\n",
      "are more manageable for the image encoder. Each instance’s cosine similarity scores at the patch level are calculated\n",
      "independently against every text prompt within the latent space. Following this, instance-level scores are combined to\n",
      "generate slide-level scores using a permutation-invariant operator, similar to those in multiple instance learning, such as\n",
      "mean or top K pooling. An optional spatial smoothing step aggregates the information of neighboring patches. When\n",
      "tested on three different real-world cancer subtyping tasks, MI-Zero either matched or outperformed baselines, achieving an\n",
      "average median zero-shot accuracy of 70.2%.\n",
      "Figure 4: Schematic of MI-Zero [36]. A gigapixel WSI is transformed into a set of patches (instances), with each patch\n",
      "being embedded into an aligned visual-language latent space. where the similarity scores between the embeddings of patches\n",
      "and the embeddings of prompts are combined using a permutation-invariant operation like topK max-pooling to generate the\n",
      "classification prediction at the WSI level.\n",
      "8Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision\n",
      "In another study, Bannur et al. [ 32] unveiled the BioViL-T method for biomedical Vision-Language Processing (VLP).\n",
      "Exploiting the data’s temporal structure, BioViL-T reached state-of-the-art levels in tasks such as progression classification,\n",
      "phrase grounding, and report generation. Incorporating prior images and reports considerably enhanced the model’s efficacy\n",
      "in disease classification and sentence-similarity tasks. The hybrid multi-image encoder in BioViL-T adeptly captured\n",
      "spatiotemporal features, proving valuable for tasks demanding dense visual reasoning over time.\n",
      "Furthermore, Tiu et al. [30] revealed the potential of self-supervised learning models in pathology detection. Their model,\n",
      "CheXzero, showcased accuracies on par with radiologists in pathology classification. Remarkably, it outdid fully supervised\n",
      "models in detecting certain pathologies and demonstrated adaptability to unannotated pathologies, which weren’t specifically\n",
      "included during training. Such results emphasize the strength of contrastive textually prompted models in deciphering\n",
      "medical image interpretation tasks from unannotated data, thus minimizing dependence on extensive labeling.\n",
      "The body of work presented emphasizes contrastive textually prompted models’ indispensable role in medical imaging.\n",
      "They showcase efficiency, performance enhancements, and an uncanny ability to infer intricate medical connotations.\n",
      "These models offer a promising solution to data scarcity, enriching medical image understanding and ultimately optimizing\n",
      "healthcare delivery.\n",
      "3.1.2 Generative\n",
      "Generative models represent another category within the domain of textually prompted models for medical imaging.\n",
      "These models are designed to generate realistic medical images based on textual prompts or descriptions. They employ\n",
      "techniques such as variational autoencoders (V AEs) and generative adversarial networks (GANs) to understand the underlying\n",
      "distribution of medical images, subsequently creating new samples that correlate with given prompts. These models have\n",
      "shown promise in tasks such as producing images of specific diseases, augmenting training data, and crafting images that\n",
      "adhere to attributes detailed in the prompts. They offer valuable tools for data augmentation, anomaly detection, and creating\n",
      "varied medical image datasets for both training and evaluation. Nonetheless, challenges like capturing the intricacies and\n",
      "variability of medical images, maintaining semantic alignment between generated images and prompts, and addressing\n",
      "ethical concerns tied to fabricated medical images persist and warrant further research.\n",
      "In a notable study, Yan et al. [41] launched Clinical-BERT, a vision-language pre-training model fine-tuned for the medical\n",
      "sector. Pre-training encompassed domain-specific tasks like Clinical Diagnosis (CD), Masked MeSH Modeling (MMM), and\n",
      "Image-MeSH Matching (IMM). Their research demonstrated that Clinical-BERT outperformed its counterparts, especially\n",
      "in radiograph diagnosis and report generation tasks. Such results emphasize the utility of infusing domain-specific insights\n",
      "during the pre-training phase, thereby refining medical image analysis and clinical decision-making.\n",
      "Singhal et al. [42] put forth Med-PaLM 2, a state-of-the-art large language model (LLM) targeting expert competence in\n",
      "medical question answering. By blending foundational LLM enhancements with medical-specific fine-tuning and innovative\n",
      "prompting tactics, the team sought to amplify the model’s proficiency. Med-PaLM 2 exhibited remarkable progress,\n",
      "registering elevated accuracy and better alignment with clinical utility. When subjected to pairwise ranking assessments,\n",
      "medical practitioners even favored Med-PaLM 2’s responses over those of their peers in terms of clinical relevance. This\n",
      "progression signifies the budding potential of LLMs in the realm of medical inquiries, inching closer to rivaling human\n",
      "physicians.\n",
      "Moor et al. [43] delved into the creation of Med-Flamingo, a few-shot learner with multimodal capabilities, tailor-made\n",
      "for medical applications. The model underwent pre-training on synchronized and staggered medical image-text data,\n",
      "followed by performance assessment on challenging visual question-answering (VQA) datasets. The outcome revealed that\n",
      "Med-Flamingo augmented generative medical VQA performance by up to 20%, as per clinician evaluations. Moreover, the\n",
      "model demonstrated prowess in addressing intricate medical queries and furnishing comprehensive justifications, surpassing\n",
      "preceding multimodal medical foundational models. These revelations underscore Med-Flamingo’s potential to enrich\n",
      "medical AI paradigms, promote personalized medicine, and bolster clinical decisions.\n",
      "Collectively, these investigations showcase the strides made in the realm of generative textually prompted models and\n",
      "their implications for the medical sector. Merging domain-specific insights with advancements in language models and\n",
      "multimodal learning techniques has yielded auspicious results in areas like radiograph diagnosis, medical question resolution,\n",
      "and generative medical VQA. Such pioneering works fortify the burgeoning research landscape and chart the course for\n",
      "future innovations in generative models tailored for healthcare applications.\n",
      "3.1.3 Hybrid\n",
      "Hybrid textually prompted models distinguish themselves through the integration of training paradigms, specifically\n",
      "leveraging both generative and contrastive methodologies.\n",
      "9Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision\n",
      "In a notable study, Chen et al. [28] unveiled a streamlined computer-aided diagnosis (CAD) system tailored for a specific\n",
      "3D imaging modality, MRI. Drawing inspiration from BLIP-2 [ 85], they crafted a language-image pre-training model\n",
      "that employs bootstrapping to amalgamate 3D medical images with textual data via a query mechanism. At the outset,\n",
      "the researchers deployed a patch embedding that was trainable, bridging the disparity between 3D medical images and\n",
      "a previously trained image encoder. This approach markedly diminished the volume of image data requisite for training.\n",
      "Following this, they introduced the MedQFormer, an innovation that harnesses adjustable queries to align visual attributes\n",
      "seamlessly with the linguistic features demanded by a language model. To round off their methodology, they chose\n",
      "BioMedLM [86] as the foundational language model and fine-tuned it by harnessing the LoRA technique [87].\n",
      "An exhaustive suite of experiments, encompassing over 30,000 image volumes sourced from five public Alzheimer’s disease\n",
      "(AD) datasets, affirmed the model’s prowess. The results spotlighted its proficiency in zero-shot classification, distinguishing\n",
      "healthy individuals, subjects with mild cognitive impairment (MCI), and those diagnosed with AD. This efficacy underscores\n",
      "the model’s potential in executing medical visual question-answering (VQA) tasks with precision.\n",
      "3.1.4 Conversational\n",
      "Conversational textually prompted models aim to enable interactive dialogues between medical professionals and the model\n",
      "by fine-tuning the foundational models on specific instruction sets. These models facilitate communication and collaboration\n",
      "between humans and the model, allowing medical experts to ask questions, provide instructions, or seek explanations\n",
      "regarding medical images. By incorporating conversational capabilities, these models enhance the interpretability and\n",
      "usability of foundational models in medical imaging. Researchers have explored various techniques to fine-tune the models\n",
      "on conversational datasets and develop architectures that can effectively process textual prompts in a dialogue context.\n",
      "Conversational textually prompted models hold great potential in medical imaging, enabling improved communication,\n",
      "knowledge transfer, and decision-making processes among medical professionals and AI systems. However, challenges\n",
      "related to understanding context, handling ambiguous queries, and ensuring accurate responses in complex medical scenarios\n",
      "are areas that require further investigation and refinement.\n",
      "In the study conducted by Li et al. [51], a cost-efficient approach for training a vision-language conversational assistant for\n",
      "biomedical images was introduced. The researchers leveraged a large-scale biomedical figure-caption dataset and utilized\n",
      "GPT-4 to generate instructions from text alone. By fine-tuning a general-domain vision-language model using a curriculum\n",
      "learning method, they developed the LLaV A-Med model. The findings showed that LLaV A-Med outperformed previous\n",
      "state-of-the-art models on certain metrics in three standard biomedical visual question-answering datasets. This highlights\n",
      "the potential of Conversational Textually Prompted Models, such as LLaV A-Med, in assisting with inquiries and answering\n",
      "open-ended research questions about biomedical images.\n",
      "Another study, conducted by Thawkar et al. [ 52], focused on the development of XrayGPT, a conversational medical\n",
      "vision-language model designed specifically for analyzing chest radiographs. XrayGPT aligned a medical visual encoder\n",
      "(MedClip) with a fine-tuned large language model (Vicuna) to enable visual conversation abilities grounded in a deep\n",
      "understanding of radiographs and medical knowledge. The study found that XrayGPT demonstrated exceptional visual\n",
      "conversation abilities and a deep understanding of radiographs and medical domain knowledge. Fine-tuning the large\n",
      "language model on medical data and generating high-quality summaries from free-text radiology reports further improved\n",
      "the model’s performance. These findings highlight the potential of Conversational Textually Prompted Models like XrayGPT\n",
      "in enhancing the automated analysis of chest radiographs and aiding medical decision-making.\n",
      "The study by Shu et al. [ 57], introduced Visual Med-Alpaca, an open-source parameter-efficient biomedical foundation\n",
      "model that combines language and visual capabilities. Visual Med-Alpaca was built upon the LLaMa-7B architecture and\n",
      "incorporated plug-and-play visual modules. The model was trained using a curated instruction set generated collaboratively\n",
      "by GPT-3.5-Turbo and human experts. The findings showed that Visual Med-Alpaca is a parameter-efficient biomedical\n",
      "model capable of performing diverse multimodal biomedical tasks. Incorporating visual modules and using cost-effective\n",
      "techniques like Adapter, Instruct-Tuning, and Prompt Augmentation made the model accessible and effective. This study\n",
      "emphasizes the importance of domain-specific foundation models and demonstrates the potential of conversational textually\n",
      "prompted models like Visual Med-Alpaca in biomedical applications.\n",
      "3.2 Visually Prompted Models\n",
      "Within medical imaging, the recent surge of visually prompted models promises a blend of precision, adaptability, and\n",
      "generalization. These models, informed by the extensive capabilities of foundation models, offer the potential to revolutionize\n",
      "medical image analysis by catering to specific tasks while also adapting to a vast array of modalities and challenges. This\n",
      "section delves into two main trajectories of such models:\n",
      "1. Adaptations: As the name suggests, this sub-section explores the adaptations and modifications made to traditional\n",
      "segmentation models, enhancing their specificity and performance for medical imaging tasks. From models\n",
      "10Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision\n",
      "that augment SAM’s capabilities for medical images to frameworks that synergize few-shot localization with\n",
      "segmentation abilities, we traverse the journey of various innovations in the realm of medical image segmentation.\n",
      "2. Generalist: Moving beyond task-specific adaptability, the models in this sub-section embody the essence of a\n",
      "’Generalist’ approach. They are designed to encompass a broader spectrum of tasks and data modalities. These\n",
      "models not only process different kinds of medical imaging data but also can integrate patient histories and genomic\n",
      "data, marking a stride towards a more holistic healthcare technology ecosystem.\n",
      "As we delve deeper into this section, we will uncover the transformative potential of visually prompted models in medical\n",
      "imaging, highlighting both their specialized adaptations and their expansive generalist capabilities.\n",
      "Figure 5: The SAM-Med2D pipeline [ 11] involves freezing the image encoder and introducing learnable adapter layers\n",
      "within each Transformer block to assimilate domain-specific expertise in the medical domain. The prompt encoder is\n",
      "fine-tuned using point, Bbox, and mask information, with the mask decoder’s parameters being updated through interactive\n",
      "training.\n",
      "3.2.1 Adaptations\n",
      "Traditional medical image segmentation has primarily relied on task-specific models, which, while accurate in their\n",
      "domains, often lack the ability to generalize across multiple tasks and imaging modalities. This necessitates a tailored,\n",
      "resource-intensive approach for each segmentation challenge. The advent of foundation models trained on extensive datasets\n",
      "presents an exciting solution. These models are capable of recognizing and segmenting numerous anatomical structures and\n",
      "pathological lesions across different imaging modalities. However, despite their potential, there are challenges with existing\n",
      "models like SAM, especially when applied to medical images [46]. This necessitates further innovations to extend their\n",
      "capabilities, and one such approach is the Medical SAM Adapter, which bridges the gap and enhances SAM’s performance\n",
      "in the medical domain [46]. This promises an integration of automated processes with specific customization.\n",
      "Ma and Wang presented MedSAM, a novel foundation model crafted for medical image segmentation [12]. Built using\n",
      "a comprehensive dataset of over a million medical image-mask pairs, MedSAM can address numerous segmentation\n",
      "tasks across various imaging modalities. Its promptable configuration seamlessly blends automation with user-driven\n",
      "customization. MedSAM excelled in tasks, especially in computing pivotal biomarkers like accurate tumor volume in\n",
      "oncology. However, it had some limitations, such as modality representation imbalances in its training data and challenges\n",
      "in segmenting vessel-like structures. Nevertheless, its architecture permits future refinements to cater to specific tasks,\n",
      "emphasizing the adaptability of foundation models in medical image segmentation.\n",
      "Lei et al. tackled the challenge of the intensive annotation workload inherent in the SAM, by introducing MedLSAM, a\n",
      "novel framework that synergizes few-shot landmark localization with SAM’s segmentation capabilities [13]. MedLSAM\n",
      "framework consists of a Localization Anything Model (MedLAM), which employs a shared Pnet to transform support\n",
      "and query patches into 3D latent vectors. During inference, MedLAM initiates with a randomly positioned agent in the\n",
      "query image and guides it toward the target landmark. The agent’s trajectory is updated based on a 3D offset computed\n",
      "from the MedLAM model, effectively localizing the landmark coarsely within the query image. This coarse localization\n",
      "is further refined using the Multi-Scale Similarity (MSS) component, enhancing the accuracy of landmark positioning\n",
      "significantly. Having localized the landmarks, the framework transitions to segmentation using both SAM and MedSAM, a\n",
      "11Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision\n",
      "specialized version of SAM fine-tuned for medical images. Trained on an extensive dataset of 14,012 CT scans, MedLSAM\n",
      "autonomously generates 2D bounding boxes across slices, facilitating SAM’s segmentation tasks. Impressively, it equaled\n",
      "SAM’s performance on two 3D datasets that spanned 38 organs but with significantly fewer annotations. Future-proofed\n",
      "with forward compatibility, MedLSAM opens doors for integration with evolving 3D SAM models, signaling even more\n",
      "effective segmentation in the medical domain.\n",
      "Gong et al. tackled the challenges posed by SAM, originally for 2D natural images when applied to 3D medical image\n",
      "segmentation, particularly for tumor detection [88]. The team introduced a strategy transforming SAM for 3D medical\n",
      "imaging while retaining most of its pre-trained parameters. By employing a visual sampler for the prompt encoder and a\n",
      "lightweight mask decoder emphasizing multi-layer aggregation, the resulting model, the 3DSAM-adapter, exhibited superior\n",
      "performance. It outperformed leading medical segmentation models in three of four tasks, reaffirming the potential to\n",
      "enhance SAM’s utility in intricate medical imaging tasks.\n",
      "Figure 6: BiomedGPT [ 25] demonstrates its versatility in various tasks through pretraining, including unimodal and\n",
      "multimodal approaches, and incorporates object detection for location data. After pretraining, it excels in five downstream\n",
      "tasks, showcasing its data efficiency.\n",
      "Cheng et al. introduced SAM-Med2D, a specialized model for 2D medical image segmentation [ 11]. Recognizing\n",
      "the need for domain adaptation, they amassed a substantial dataset of approximately 4.6M images and 19.7M masks,\n",
      "spanning diverse medical modalities. A notable feature of SAM-Med2D is its varied prompt strategies, going beyond\n",
      "bounding boxes and points to incorporate masks, offering a comprehensive interactive segmentation approach as shown in\n",
      "Figure 5. Thorough evaluations showcased its superior performance across various anatomical structures, with remarkable\n",
      "generalization capabilities proven on datasets from the MICCAI 2023 challenge. Despite its prowess, certain challenges\n",
      "remain, particularly with complex boundaries and low-contrast objects. With prospects of integrating natural language\n",
      "interaction, SAM-Med2D stands as a pioneering contribution to medical computer vision research. Building upon the theme\n",
      "of customization, another noteworthy effort is the development of SAMed. This model, unlike its predecessors, employs a\n",
      "low-rank-based finetuning strategy, enabling it to perform semantic segmentation on medical images with only a fraction of\n",
      "SAM’s parameters being updated. This selective approach to parameter adaptation allows SAMed to achieve competitive\n",
      "results, underscoring the potential of customizing large-scale models for specific medical segmentation tasks [47].\n",
      "In a stride to enhance reliability in medical image segmentation, Deng et al. put forth SAM-U [ 44], a novel approach\n",
      "employing multi-box prompts for refined uncertainty estimation in SAM predictions. This method significantly improves\n",
      "SAM’s performance, especially in low-quality medical images, and provides crucial insights through generated uncertainty\n",
      "maps, highlighting potential segmentation inaccuracies and serving as an essential guide for clinicians in areas requiring\n",
      "manual annotations. This innovative approach underscores the advancements and adaptability in the realm of medical image\n",
      "segmentation\n",
      "12Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision\n",
      "3.2.2 Generalist\n",
      "In contrast to their adaptability to specific tasks through prompts, foundational models also offer a ’Generalist’ approach,\n",
      "further disrupting the landscape of medical imaging. These Generalist models expand upon the foundational model capabili-\n",
      "ties by being intrinsically designed to handle a broader spectrum of medical imaging tasks and data modalities—ranging\n",
      "from X-rays to MRIs, and even incorporating patient histories and genomic data. The key advantage here is their capability\n",
      "for dynamic task specification, often enabled by natural language descriptions, obviating the need for model retraining.\n",
      "This inherent flexibility is further augmented by the models’ ability to formally represent medical knowledge, allowing for\n",
      "reasoned outputs and explanations. The emergence of Generalist models in medical imaging signifies a step towards a more\n",
      "integrated and efficient healthcare technology ecosystem.\n",
      "Moor et al. [ 23] delve into the intricacies of developing General-purpose Medical Artificial Intelligence (GMAI), a\n",
      "specialized class of foundation models optimized for the healthcare domain. Unlike conventional medical AI, GMAI models\n",
      "are designed to process multiple data modalities, such as imaging studies and electronic health records, simultaneously.\n",
      "These models are not only capable of complex diagnostic tasks but can also generate treatment recommendations complete\n",
      "with evidence-based justifications. The authors discuss challenges unique to GMAI, including the need for multi-disciplinary\n",
      "panels for output verification and increased susceptibility to social biases due to the complex training data sets. Additionally,\n",
      "they raise concerns over patient privacy and the computational and environmental costs associated with model scaling. The\n",
      "paper underscores that the success of GMAI hinges on rigorous validation and ongoing oversight to mitigate these risks\n",
      "while harnessing its transformative potential in healthcare.\n",
      "Tu et al. extend the pioneering work of Med-PaLM and Med-PaLM2 [42] to introduce Med-PaLM M, a groundbreaking\n",
      "multi-modal biomedical AI system capable of handling diverse medical modalities, including medical imaging, genomics,\n",
      "and electronic health records [22]. Building upon the foundational achievements of Med-PaLM—which was the first AI\n",
      "to surpass the pass mark on USMLE-style questions—and the subsequent improvements in Med-PaLM2, which boasted\n",
      "an accuracy of 86.5% on the same questions, Med-PaLM M employs a fusion of Vision Transformer (ViT) for visual\n",
      "tasks and Language-agnostic Language Model (LLM) for natural language tasks. These components are fine-tuned on a\n",
      "newly assembled MultiMedBench dataset. Med-PaLM M eclipses existing benchmarks, including specialized single-task\n",
      "models and its predecessor generalist models like PaLM-E that lacked biomedical fine-tuning. Notably, the system exhibits\n",
      "unprecedented zero-shot learning capabilities, successfully identifying tuberculosis from chest X-ray images without prior\n",
      "training [24]. It also excels in generating radiology reports, rivaling the performance of expert radiologists in human\n",
      "evaluations. While the study highlights the scalability and promise of multi-modal AI models for a range of biomedical\n",
      "tasks, it also acknowledges existing challenges, such as data scarcity and limitations of current benchmarks. The work\n",
      "serves as a seminal contribution, marking a new frontier in biomedical AI, albeit with cautionary notes on safety and equity\n",
      "considerations for real-world applications.\n",
      "Zhang et al. introduce BiomedGPT [ 25], a unified framework that is trained across multiple modalities—including\n",
      "radiographs, digital images, and text—to perform a diverse range of tasks in the biomedical domain as shown in Figure 6.\n",
      "The model particularly excels in image classification on MedMNIST v2 datasets and visual question-answering on SLAKE\n",
      "and PathVQA, setting new state-of-the-art benchmarks. However, it lags in text-based tasks such as natural language\n",
      "inference on the MedNLI dataset. One reason for this performance gap is the model’s constrained scale; with only 182\n",
      "million parameters, it is smaller than other state-of-the-art models. The study also pinpoints the model’s sensitivity to task\n",
      "instructions and challenges with handling out-of-distribution data as areas for future research. Nonetheless, BiomedGPT\n",
      "represents a significant step towards a versatile, generalist model in the biomedical field, capable of both vision and language\n",
      "tasks.\n",
      "Wu et al. introduce the Radiology Foundation Model (RadFM) and the MedMD dataset, aiming to unify medical tasks\n",
      "and integrate diverse radiological images [26]. RadFM effectively merges medical scans with natural language, addressing\n",
      "various medical tasks. The study unveils RadBench, a benchmark demonstrating RadFM’s superior synthesis of visual and\n",
      "textual information. Despite the advancements, the authors highlight limitations, such as the prevalence of 2D images in\n",
      "the dataset and challenges in generating clinically useful sentences. Wu et al.’s release of these innovations significantly\n",
      "advances radiological models, encourages collaborative progress, and emphasizes the need for enhanced evaluative metrics\n",
      "and comprehensive solutions in the field.\n",
      "In another study, Zhou et al. introduce RETFound [27], a versatile foundation model developed through self-supervised\n",
      "learning, trained on 1.6 million unlabeled retinal images. It demonstrates unparalleled adaptability and generalizability\n",
      "in diagnosing eye diseases and predicting systemic disorders with notable accuracy and reduced reliance on extensive\n",
      "annotation. RETFound overcomes significant barriers related to data limitations and model generalization, offering a\n",
      "pioneering solution in medical AI, with the potential to democratize and significantly advance healthcare AI applications.\n",
      "13Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision\n",
      "Figure 7: Extensions of the SAM model for diverse medical image segmentation tasks [ 11]. This figure illustrates the\n",
      "versatility of SAM-based adaptations in addressing a wide range of medical image segmentation challenges, showcasing\n",
      "their applicability and adaptability across various healthcare scenarios.\n",
      "4 Discussion\n",
      "In the dynamic landscape of foundational models for medical imaging, each direction outlined in our taxonomy (Figure 3)\n",
      "brings its own set of advantages and distinctive capabilities to the forefront. These divergent paths cater to specific needs,\n",
      "creating a diversified toolkit for addressing the multifaceted challenges of the medical imaging domain. As we delve into\n",
      "this discussion, we will explore the unique advantages of each direction and consider scenarios where one direction might\n",
      "excel over the others, all while peering into how these models learn feature representations and the implications thereof.\n",
      "Textually Prompted Contrastive Models: These models have shown remarkable prowess in bridging the semantic gap\n",
      "between medical images and text. By leveraging contrastive learning, these models can extract meaningful representations\n",
      "from unpaired medical image-text data, thereby reducing the dependence on vast amounts of labeled data. This approach is\n",
      "particularly advantageous in scenarios where labeled data is scarce or expensive to obtain, such as rare medical conditions\n",
      "or specialized imaging modalities. Contrastive models excel at capturing subtle medical meanings and are well-suited for\n",
      "tasks like zero-shot prediction in medical image-text tasks. For instance, in scenarios where a new, uncharacterized medical\n",
      "condition arises, these models can adapt swiftly by simply providing textual descriptions.\n",
      "14Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision\n",
      "However, there are limitations to consider. Contrastive models might struggle with highly complex medical images or\n",
      "intricate pathologies, where the nuances demand a deeper level of feature representation. Additionally, they may still rely on\n",
      "the availability of large-scale text data, which could be a bottleneck in some cases. The contrastive learning process also\n",
      "hinges on careful tuning of hyperparameters, making it essential to invest time in fine-tuning for optimal performance.\n",
      "Textually Prompted Generative Models: Textually prompted generative models, exemplified by models like Clinical-BERT\n",
      "[41] and Med-Flamingo [43], offer the ability to generate detailed responses and explanations for medical image-related\n",
      "queries. They excel in tasks requiring a deep understanding of the medical domain, making them invaluable in clinical\n",
      "decision support systems, medical education, and generating radiology reports.\n",
      "These generative models can be a game-changer when interpretability and reasoning are crucial. For instance, in a clinical\n",
      "setting, generating explanations for a model’s predictions can enhance trust and facilitate collaboration between AI systems\n",
      "and medical professionals. In educational contexts, they can serve as powerful tutors, providing in-depth explanations and\n",
      "context.\n",
      "Nevertheless, generative models are computationally intensive and demand significant training data. They may not be the\n",
      "most efficient choice for scenarios where quick, lightweight predictions are required. Additionally, they may face challenges\n",
      "in generating text that is both informative and concise, which could be important in some applications.\n",
      "Textually Prompted Hybrid Models: Hybrid models, as represented by MedBLIP [28], combine the strengths of generative\n",
      "and contrastive methodologies. These models tackle the challenge of integrating textual data with 3D medical images, often\n",
      "a complex task due to the inherent differences in data modalities.\n",
      "One of the key advantages of hybrid models is their potential for zero-shot prediction in medical image-text tasks. They\n",
      "seamlessly align visual attributes with linguistic features, making them adept at executing medical visual question-answering\n",
      "tasks with precision. For example, in cases where medical professionals need to quickly diagnose conditions based on both\n",
      "images and textual descriptions, hybrid models can provide valuable support.\n",
      "Yet, hybrid models may face challenges related to the design of effective integration mechanisms between textual and visual\n",
      "data. The success of these models often relies on the quality of the alignment between different modalities. Additionally,\n",
      "they may require substantial computational resources for training and fine-tuning.\n",
      "Textually Prompted Conversational Models: Conversational models, like LLaV A-Med [51] and XrayGPT [ 52], are\n",
      "designed to enable interactive dialogues between medical professionals and AI systems. These models are particularly\n",
      "beneficial in scenarios where medical experts need to ask questions, seek explanations, or instruct AI systems regarding\n",
      "medical images. One of the most significant advantages of conversational models is their potential to enhance communication\n",
      "and collaboration between humans and AI. They can facilitate knowledge transfer, clarify doubts, and provide detailed\n",
      "explanations for complex medical images. In a clinical context, this can lead to more informed decision-making and better\n",
      "patient care.\n",
      "However, conversational models face the challenge of understanding context and handling ambiguous queries effectively.\n",
      "Ensuring accurate responses in complex medical scenarios remains an ongoing research challenge. Additionally, they\n",
      "require careful fine-tuning on conversational datasets to perform optimally.\n",
      "Visually Prompted Adaptations Models: Visually prompted adaptations models, such as MedLSAM [13], MedLSAM,\n",
      "3DSAM-adapter [88], SAM-Med2D [11], SAMed [47], and SAM-U [44], focus on enhancing the specificity and perfor-\n",
      "mance of medical image segmentation tasks. These models adapt foundational models like SAM for the medical domain,\n",
      "addressing challenges like data scarcity and complex boundaries. A sample of segmentation results on various medical image\n",
      "analysis tasks achieved through the adapted SAM model is presented in Figure 7, showcasing its remarkable achievement in\n",
      "diverse medical imaging scenarios and its robust generalization power.\n",
      "15Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision\n",
      "Table 1: Overview of the reviewed Foundation models in medical imaging based on their algorithm choice presented in our\n",
      "taxonomy, Figure 3.\n",
      "Algorithm Networks Core Ideas Practical Use Cases\n",
      "Textually\n",
      "Prompted\n",
      "Contrastive\n",
      "Models\n",
      "1MedClip [31]2BioViL-T [32]3CheXzero [30]4MI-Zero [36]\n",
      "•[31] aims to tackle medical image and report vision-text contrastive learning difficulties. MediCLIP separates medical images and text for\n",
      "multimodal contrastive learning. Scaling combinatorial training data at low cost answers medical data shortages. The study advises replacing\n",
      "InfoNCE with a medical-based semantic matching loss to eliminate contrastive learning false negatives. MedCLIP captures subtle but important\n",
      "medical meanings better than zero-shot prediction, supervised classification, and image-text retrieval methods. The paper reveals MedCLIP’s\n",
      "efficacy and data efficiency, which might enhance clinical decision-making and downstream tasks.\n",
      "•Use data temporal structure to improve biological vision-language processing (VLP) in [32]. Researchers introduce BioViL-T, a pre-training\n",
      "system that trains and fine-tunes using past pictures and data. This method uses temporal correlations and a multi-image encoder to handle\n",
      "missing images and longitudinal data without image registration. Modality alignment is improved by analyzing the temporal relationship between\n",
      "visuals and reports, enhancing pre-training and downstream task performance. The study exhibits advanced progression categorization, phrase\n",
      "grounding, and report generation results. Temporal and non-temporal tasks like pneumonia detection and phrase grounding benefit from prior\n",
      "context and temporal knowledge. To test and benchmark chest X-ray VLP models for temporal semantics, the authors offer MS-CXR-T, a\n",
      "multimodal benchmark dataset. An expert radiologist curated this dataset to measure image-text temporal correlations.\n",
      "•In [30], authors offer a novel medical imaging pathological classifying approach. The study suggests employing self-supervised learning\n",
      "without annotations to accurately diagnose illnesses in unannotated chest X-rays. Large labeled datasets are expensive and time-consuming for\n",
      "traditional medical image interpretation machine-learning algorithms. This research shows that a self-supervised system trained on chest X-rays\n",
      "without annotations can classify illness as well as radiologists. A zero-shot multi-label classification method, natural language supervision\n",
      "from radiology reports, and generalization to diverse image interpretation tasks and datasets are presented in the research. CheXzero learns a\n",
      "representation for zero-shot multi-label classification without labeled data fine-tuning using contrastive learning with image-text pairs. Radiology\n",
      "reports’ natural labeling lets self-supervised algorithms perform as well as professional radiologists and fully supervised approaches on unknown\n",
      "disorders. This approach eliminates explicit labeling, eliminating medical machine-learning workflow inefficiencies from large-scale labeling.\n",
      "•Zero-shot prediction in medical image-text tasks, Supervised classification in medical\n",
      "image analysis, Image-text retrieval in the medical domain, Supporting clinical decision-\n",
      "making and downstream clinical tasks [31].\n",
      "•Progression Classification: Achieving State-of-the-Art Performance in Tracking\n",
      "Medical Condition Progression, Phrase Grounding: Linking Clinical Report Phrases to\n",
      "Image Regions for Enhanced Analysis, Report Generation: Improved Performance by\n",
      "Incorporating Prior Reports, Disease Classification: Consistent Improvement in Disease\n",
      "Classification Tasks, Pneumonia Detection: State-of-the-Art Results in Detecting\n",
      "Pneumonia [32]\n",
      "•Automation of complex medical image interpretation tasks, Disease diagnosis,\n",
      "Diagnostic efficiency improvement, Label efficiency enhancement, Decreased reliance\n",
      "on large labeled datasets, Reduction in labeling efforts and costs, Potential for learning\n",
      "a broad range of medical image interpretation tasks from unlabeled data [30]\n",
      "•Zero-shot transfer for cancer subtype classification on 3 WSI datasets. Moreover, the\n",
      "curated dataset of histopathology image-caption pairs can potentially be generalized\n",
      "and adapted to develop practical solutions in other domains [36].\n",
      "Textually\n",
      "Prompted\n",
      "Generative\n",
      "Models\n",
      "1Clinical-BERT [41]2Med-PaLM 2 [42]3Med-Flamingo [43]\n",
      "•Clinical-BERT [41], a medical pre-training paradigm, underpins. The research offers domain-specific pre-training activities, including Clinical\n",
      "Diagnosis (CD), Masked MeSH Modeling (MMM), and Image-MeSH Matching for model training. MeSH words in radiograph reports are\n",
      "stressed. The work aligns MeSH terms with radiographs using region and word sparse attention. The model links visual characteristics with\n",
      "MeSH phrases using this attention mechanism. Clinical-BERT radiograph diagnostic and report production provide cutting-edge results. The\n",
      "article shows domain-specific pre-training exercises and MeSH keywords to improve medical task performance.\n",
      "•Expert medical question answering is done using LLMs in [42]. The study aims to enhance LLM performance to match model and clinician\n",
      "replies. The authors say LLMs have advanced in various disciplines and can address medical questions. They admit prior LLM-based models\n",
      "need to be improved, especially compared to clinician responses. The authors offer various LLM performance enhancements. Base LLM\n",
      "improvements (PaLM 2), medical domain-specific fine-tuning, and a new ensemble refinement approach are used. The strategies aim to enhance\n",
      "medical thinking and results.\n",
      "•Med-Flamingo [43], a vision-language model suggested is pre-trained on medical image-text data from various sources and can create\n",
      "open-ended replies from textual and visual input. Med-Flamingo outperforms prior models in generative medical visual question-answering\n",
      "tasks by 20% in clinical assessment scores due to in-context learning The research also describes Visual USMLE, a difficult created VQA dataset\n",
      "including medical questions, images, and case vignettes. The paper says multimodal few-shot and in-context learning improve medical AI\n",
      "models.\n",
      "•Radiograph Diagnosis and Reports Generation: Achieving state-of-the-art results on\n",
      "challenging datasets, Enhancing Downstream Tasks in the Medical Domain, Improving\n",
      "performance in various medical domain tasks, Learning Medical Domain Knowledge:\n",
      "Enabling the model to acquire domain-specific knowledge for better performance [41]\n",
      "•Medical question answering: Providing accurate and reliable answers to medical\n",
      "questions. Medical exams: Assisting in preparing for medical licensing examinations.\n",
      "Clinical decision support: Aiding physicians in making informed decisions during\n",
      "patient care. Consumer health information: Delivering trustworthy medical information\n",
      "to the general public [42].\n",
      "•Generative Medical Visual Question Answering (VQA), Medical Reasoning and\n",
      "Rationale Generation, Clinical Evaluation and Human Rater Study, Dataset Creation\n",
      "for Pre-training and Evaluation [43]\n",
      "Textually\n",
      "Prompted\n",
      "Hybrid Mod-\n",
      "els\n",
      "1MedBLIP [28] •Extend a 2D image encoder to extract features from 3D medical images and obtain a lightweight language model for our CAD purpose.\n",
      "•Align different types of medical data into the common space of language models, besides collecting the largest public dataset for studying\n",
      "Alzheimer’s disease (AD).\n",
      "•Zero-shot prediction in medical image-text tasks\n",
      "•Zero-shot medical visual question answering (VQA) which involves producing an\n",
      "initial diagnosis for an unseen case by analyzing input images and textual descriptions,\n",
      "while also offering explanations for the decision-making process.\n",
      "Textually\n",
      "Prompted\n",
      "Conversa-\n",
      "tional Models\n",
      "1LLaV A-Med [51]2XrayGPT [52]3Visual Med-Alpaca [57]4PMC-LLaMA[89]5ClinicalGPT [50]6Radiology-LLamA2 [53]\n",
      "•Training a low-cost vision-language conversational assistant for biological imagery is the main notion of [51]. The authors recommend\n",
      "training the computer using a big PubMed Central biomedical figure-caption dataset. Caption data and a novel curriculum learning process\n",
      "let GPT-4 self-instruct open-ended education. The model can align biological vocabulary using figure-caption pairings and grasp open-ended\n",
      "conversational semantics. This strategy resembles how laypeople absorb biological topics. LLaV A-Med can answer biological picture inquiries\n",
      "and has great multimodal communication skills. Fine-tuning LLaV A-Med outperforms supervised biomedical visual question answering in this\n",
      "investigation. The paper releases instruction-following data and the LLaV A-Med model for biomedical multimodal learning research.\n",
      "•XrayGPT [52], a conversational medical vision-language model, answers open-ended chest radiograph questions. The model uses MedClip’s\n",
      "visual characteristics and Vicuna’s textual information to assess radiographs and medical domain knowledge. Interactive and high-quality\n",
      "free-text radiology report summaries enhance XrayGPT automated chest radiograph processing. XrayGPT domain-specific information may\n",
      "enhance chest radiograph analysis.\n",
      "•[57] proposes “visual medical specialists\" for multimodal biological activities. Training the model using GPT-3.5 Turbo and human experts\n",
      "use instruction-tuning. Plug-and-play visual modules integrate text and vision for multimodal applications. Visual Med-Alpaca is open-source\n",
      "and cheap for doctors.\n",
      "•[89] uses an open-source medical language model. Through medical expertise, the study proposes a logical strategy to adapt a general-purpose\n",
      "language paradigm to medicine. The language model contains 4.8 million biomedical academic papers and 30,000 medical textbooks. Medical\n",
      "accuracy is improved by fine-tuning the model to domain-specific instructions. Language model reasoning is improved by the paper. The model\n",
      "improves medical judgments by applying medical expertise to case facts and offering well-justified recommendations. Improvement of the\n",
      "language model’s alignment ability to adapt to different tasks without task-specific training is also stressed.\n",
      "•To improve NLP, [50] suggests pre-training and fine-tuning huge language models. Factual errors and a lack of medical language model\n",
      "experience are admitted. A clinical-optimized language model, ClinicalGPT, overcomes these concerns. ClinicalGPT training combines medical\n",
      "records, domain-specific knowledge, and multi-round discussions. This method offers ClinicalGPT context and expertise for clinical tasks. With\n",
      "medical knowledge question-answering, tests, patient consultations, and medical record diagnostic analysis, the study provides a complete\n",
      "evaluation system. This approach assesses ClinicalGPT’s medical performance. ClinicalGPT improves with parameter-efficient fine-tuning. For\n",
      "clinical use, these methods improve model parameters. For huge language models in healthcare, ClinicalGPT outperforms others.\n",
      "•[53] aligns the model with task-specific user objectives, develops radiology-specific language models, evaluates and improves generated\n",
      "impressions and shows the model’s better clinical impression-generating performance over other generative language models. The paper says\n",
      "that personalized language models can automate radiology jobs and improve human competency.\n",
      "•Multimodal Conversational Assistant: LLaV A-Med demonstrates excellent multi-\n",
      "modal conversational capability and can assist with inquiries about biomedical images,\n",
      "Biomedical Visual Question Answering (VQA): LLaV A-Med outperforms previous\n",
      "state-of-the-art methods on certain metrics for biomedical VQA tasks, Empowering\n",
      "Biomedical Practitioners: The proposed approach empowers biomedical practitioners\n",
      "by providing assistance with open-ended research questions and improving their under-\n",
      "standing of biomedical images [51]\n",
      "•Automated Analysis: XrayGPT enables automated analysis of chest radiographs,\n",
      "Concise Summaries: XrayGPT provides concise summaries highlighting key findings\n",
      "and overall impressions, Interactive Engagement: Users can engage interactively by\n",
      "asking follow-up questions to XrayGPT, Clinical Decision Support: XrayGPT assists\n",
      "medical professionals in making clinical decisions and provides valuable insights,\n",
      "Advancing Research: XrayGPT opens up new avenues for research in the automated\n",
      "analysis of chest radiographs [52].\n",
      "•Interpreting radiological images, Addressing complex clinical inquiries, Providing\n",
      "information on chemicals for hair loss treatment (as a case study), Supporting health-\n",
      "care professionals in diagnosis, monitoring, and treatment, Enabling prompt generation\n",
      "for specialized tasks (e.g., radiology image captioning) [57]\n",
      "Visually\n",
      "Prompted\n",
      "Adaptations\n",
      "Models\n",
      "1MedSAM [12]2MedLSAM [13]33DSAM-adapter [88]4SAM-Med2D [11]5SAMed [47]6SAM-U [44]\n",
      "•Versatile and accurate delineation of anatomical structures and pathologies across various medical imaging modalities, surmounting challenges\n",
      "of modality imbalance and intricate segmentation.\n",
      "•Utilizes merged localization and segmentation with a shared 3D coordinate system for streamlined, precise 3D medical image analysis.\n",
      "•Improves accuracy in decoding spatial patterns in volumetric data through enhanced, lightweight 3D medical image segmentation, focusing\n",
      "particularly on tumors.\n",
      "•Optimized for precise 2D medical image segmentation, utilizing diverse prompts and refinements.\n",
      "•utilizes a low-rank-based finetuning strategy for specialized medical image segmentation, maintaining minimal costs and enhanced capabilities.\n",
      "•Employs multi-box prompts to refine SAM’s segmentation with pixel-level uncertainty estimation, increasing accuracy and providing nuanced\n",
      "image understanding.\n",
      "•Pivotal for a range of clinical applications including efficient segmentation, diagnosis,\n",
      "treatment planning, disease monitoring, and in oncology for accurate tumor volume\n",
      "computation, contributing to personalized patient care and improved health outcomes\n",
      "[12].\n",
      "•A versatile, scalable foundation in medical imaging, reducing annotation burdens,\n",
      "and providing accurate, automated segmentation across medical disciplines, enhancing\n",
      "diagnostic procedures [13].\n",
      "•Facilitates clinical diagnosis, treatment planning, and medical R&D through im-\n",
      "proved segmentation and serves as a blueprint for domain-specific adaptations, enhanc-\n",
      "ing medical imaging automation and processes [88].\n",
      "•Enables accurate medical image analysis, offering insights for researchers and ad-\n",
      "vancing medical computer vision and interactive segmentation [11].\n",
      "•Serves as a crucial tool in computer-assisted medical diagnoses, excelling in multi-\n",
      "organ segmentation tasks, and is fully compatible with the existing SAM system,\n",
      "offering enhanced accessibility and utility in real-world medical settings [47].\n",
      "•Valuable for providing pixel-level uncertainty estimation in segmentation, aiding pre-\n",
      "cise diagnoses, and identification of segmentation errors, especially in fundus images.\n",
      "It enriches clinical analyses and fosters the development of advanced segmentation\n",
      "methods [44].\n",
      "Visually\n",
      "Prompted\n",
      "Generalist\n",
      "Models\n",
      "1GMAI [23]2BiomedGPT [25]3Med-PalM M [22]4RadFM [26]5RETFound [27]\n",
      "•Utilizes self-supervision on diverse datasets for multifunctional medical tasks with minimal labeled data, adapting to new tasks and enabling\n",
      "dynamic interaction and advanced reasoning.\n",
      "•excels in diverse tasks with one model weight set, surpassing specialized models and offering versatile zero-shot generalization in biomedicine.\n",
      "•Utilizes multi-task pretraining for knowledge transfer to unseen data, aiming to establish new benchmarks in biomedicine.\n",
      "•Adeptly integrates and analyzes multidimensional medical scans with natural language.\n",
      "•Uses masked autoencoder techniques to identify retinal structures and patterns related to eye and systemic diseases like heart failure, showing\n",
      "high adaptability across various tasks.\n",
      "•Has potential applications in generating radiology reports and aiding medical pro-\n",
      "cedures, reducing radiologist workload through automated, contextual report drafting\n",
      "and visualization, and supporting surgical teams with real-time annotations, alerts, and\n",
      "medical reasoning, thereby improving healthcare delivery [23].\n",
      "•Efficiently conducts tasks such as image classification and report generation, sup-\n",
      "porting clinical decisions and diagnostics. It serves versatile medical needs, offering\n",
      "reliable interpretations of diverse biomedical data, particularly where specialized mod-\n",
      "els are unattainable and integrated insights are vital [25].\n",
      "•Uncovers insights essential for healthcare advancements by integrating information\n",
      "from various medical fields for diverse applications and analyses, requiring no finetun-\n",
      "ing modifications, and efficiently solving real-world problems [22].\n",
      "•It integrates multiple images, essential for longitudinal follow-ups and diverse sce-\n",
      "narios, aiding professionals in generating accurate, context-rich reports and plans by\n",
      "understanding both visual and textual medical data [26].\n",
      "•Useful in clinical settings for early detection and risk assessment, it offers a data-\n",
      "efficient solution, minimizing annotation efforts and promoting broader implementation\n",
      "in varied clinical applications, contributing to the democratization of advanced health-\n",
      "care AI technologies [27].\n",
      "16Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision\n",
      "The primary advantage of adaptation models is their ability to excel in specialized medical image segmentation tasks. For\n",
      "instance, in scenarios where precise tumor volume calculation is critical, models like MedSAM can provide accurate results.\n",
      "These models are tailored for the medical domain, making them well-suited for specific clinical applications.\n",
      "Nonetheless, adaptation models may require substantial annotated data for fine-tuning. They might face challenges in\n",
      "scenarios with limited labeled data, as achieving the desired level of performance could be challenging. Additionally, they\n",
      "might not be the most efficient choice for tasks that require generalization across diverse medical imaging modalities.\n",
      "Visually Prompted Generalist Models: Visually prompted generalist models, exemplified by models like BiomedGPT [25]\n",
      "and Med-PalM M [22], offer versatility by handling a wide spectrum of medical imaging tasks and data modalities. They\n",
      "can seamlessly switch between tasks without the need for extensive retraining, making them suitable for dynamic healthcare\n",
      "environments.\n",
      "The key advantage of generalist models is their flexibility. In scenarios where medical professionals need a single model that\n",
      "can handle various tasks, such as image classification, text generation, and question-answering, these models shine. Their\n",
      "ability to reason across different modalities and provide informed responses is invaluable in clinical decision support and\n",
      "medical research.\n",
      "However, generalist models might face challenges related to task-specific fine-tuning. Achieving state-of-the-art performance\n",
      "in highly specialized tasks might require additional domain-specific data. Moreover, these models need robust mechanisms\n",
      "for handling out-of-distribution data effectively.\n",
      "In conclusion, the choice between these directions largely depends on the specific use case and requirements. While\n",
      "textually prompted models excel in tasks requiring interpretation and detailed explanations, visually prompted models\n",
      "dominate in segmentation and image-specific tasks. Conversational models bridge the gap between human experts and AI\n",
      "systems, facilitating collaborative decision-making. The choice ultimately boils down to the nature of the problem, the\n",
      "availability of data, and the need for adaptability or versatility in the medical imaging domain. Each direction contributes\n",
      "to the evolving landscape of foundational models, offering a rich tapestry of tools to tackle diverse healthcare challenges.\n",
      "To facilitate comprehension, we have presented the benefits, drawbacks, and practical applications of each direction in\n",
      "Table 1. We also showcase the timeline of the reviewed papers in the past quarters, as illustrated in Figure 8. This figure\n",
      "presents a chronological overview of the key milestones and developments in the field, highlighting the rapid evolution and\n",
      "growing significance of textually prompted and visually prompted foundation models in medical imaging. Expanding on\n",
      "this, the timeline provides valuable insights into the progression of research, revealing the emergence of novel techniques,\n",
      "conversational, and generalist models.\n",
      "2023\n",
      "Q3\n",
      "CheXzero9\n",
      "Clinical-BERT21\n",
      "... 2022\n",
      "... Q3\n",
      "2023\n",
      "Q1\n",
      "2023\n",
      "Q4\n",
      "2023\n",
      "Q2\n",
      "2022\n",
      "Q4\n",
      "TPM: Textually Prompted Models \n",
      "VPM: Visually Prompted Models TPM - Contrastive\n",
      "TPM - Generative\n",
      "TPM - Hybrid\n",
      "TPM - Conversational\n",
      "VPM - Adaptations\n",
      "MedCLIP10\n",
      "ClinicalGPT33\n",
      "BioViL-T11 PMC-LLaMA32 \n",
      "LLaVA-Med34\n",
      "XrayGPT35\n",
      "BiomedGPT3\n",
      "MedBLIP7\n",
      "Med-PaLM 222\n",
      "SAM-B-ZSS1\n",
      "MedSAM24\n",
      "Med-Flamingo23\n",
      "SAM-U25\n",
      "Med-PalM M4\n",
      "SAM-Med2D26\n",
      "RadFM5\n",
      "Radiology-Llama236\n",
      "RETFound6\n",
      "CLIPDM-OTS12\n",
      "ChatCAD37\n",
      "VLM for VQA in MI8\n",
      "DeID-GPT38\n",
      "ChatDoctor39\n",
      "MaCo19\n",
      "ELIXR18\n",
      "KoBo16\n",
      "WebPLIP17\n",
      "VPM - Generalist\n",
      "BiomedCLIP13\n",
      "MI-Zero15\n",
      "Virchow31\n",
      "CITE20\n",
      "LVM-Med27\n",
      "SAMed30\n",
      "PTUnifier14\n",
      "Figure 8: Timeline of advancements in textually and visually prompted foundation models in medical imaging over the\n",
      "past quarters.\" This timeline illustrates significant progress and breakthroughs in the field, spanning the last five quarters,\n",
      "highlighting the dynamic nature of research and innovation in textually and visually prompted foundation models for medical\n",
      "image analysis.\n",
      "17Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision\n",
      "Table 2: A summary of publicly available information about medical foundational models, their computational demands and\n",
      "training information. The unavailable information is featured with a dash.\n",
      "ID CategorySub-categoryShort nameGPU Model Number ofGPUs GPU Memory(GB) Total GPUMemory (GB)Training Time(GPU Hour)Input Size Totalbatch size Epochs\n",
      "1 TPM ContrastiveMedCLIP Nvidia RTX 30901 24 24 8 224x224 100 10\n",
      "2 TPM ContrastiveBioViL-T Nvidia Tesla V1008 32 256 - 448x448 240 50, 100\n",
      "3 TPM ContrastiveCLIPDM-OTSNVIDIA RTX A50008 24 192 - 96x96x96 42 50\n",
      "4 TPM ContrastivePTUnifier NVIDIA A1004 80 320 - 288×288-384x38416-128 11-60\n",
      "5 TPM ContrastiveBiomedCLIPNVIDIA A10016 40 640 - 224x224-336x3364k-64k (context)40\n",
      "6 TPM ContrastiveKoBo Nvidia RTX 30902 24 48 - - 100 50\n",
      "7 TPM ContrastiveMI-Zero NVIDIA A1008 80 640 - 448x448 512 50\n",
      "8 TPM ContrastiveCITE GeForce GTX 2080 Ti2 11 22 0.37 224x224 128 (1000 iteration)\n",
      "9 TPM GenerativeClinical-BERTNvidia RTX 30902 24 48 96 224x224 256 50\n",
      "10 TPM GenerativeMed-FlamingoNvidia A100 8 80 640 1296 - 400 -\n",
      "11 TPM Hybrid MedBLIP Nvidia RTX 30901 24 24 - 224x224x2247 100\n",
      "12 TPM Hybrid VLM for VQA in MIGeForce GTX 1080 Ti1 11 11 - 224x224 50 50\n",
      "13 TPM ConversationalDeID-GPT Nvidia RTX 3090>1 24\n",
      "14 TPM ConversationalChatDoctor Nvidia A100 6 80 480 18 max-sq-len: 2048192 3\n",
      "15 TPM ConversationalPMC-LLaMANvidia A100 32 80 2560 - max-sq-len: 2048img:256, text:32008\n",
      "16 TPM ConversationalLLaV A-MedNvidia A100 8 40 320 120 - 128 100\n",
      "17 TPM ConversationalRadiology-Llama2Nvidia A100 4 80 320 - - 128 -\n",
      "18 VPM AdaptationsSAMed Nvidia RTX 30902 24 48 - 512x512 12 200\n",
      "19 VPM AdaptationsMedSAM Nvidia A100 20 80 1600 - 1024x2014 160 100\n",
      "20 VPM AdaptationsAutoSAM NVIDIA Tesla V1001 16 16 - 1024x1024 4 120\n",
      "21 VPM AdaptationsLVM-Med Nvidia A100 16 80 1280 2688 224x224 - 1024x102416 - 64 20-200\n",
      "22 VPM AdaptationsSAM-Med2DNvidia A100 8 80 640 - 256x256 - 12\n",
      "23 VPM GeneralistSAM-B-ZSSNvidia RTX 30801 10 10 - 1024x1024 1 20\n",
      "24 VPM GeneralistRadFM Nvidia A100 32 80 2560 - 256(3D), 512(2D)1(3D), 4(2D)8\n",
      "25 VPM GeneralistRETFound Nvidia A100 8 40 320 2688 16x16 16, 1792 50, 800\n",
      "4.1 Hardware Requirements and Dataset Sizes\n",
      "In the pursuit of advancing foundational models in medical imaging, it is imperative to consider the practical aspects of\n",
      "implementing these models in real-world healthcare settings. Two crucial aspects in this regard are the hardware requirements\n",
      "and dataset sizes, both of which significantly influence the feasibility and scalability of deploying these models.\n",
      "Hardware Requirements: Many foundational models, owing to their immense complexity and scale, have substantial\n",
      "hardware requirements. While these models deliver remarkable performance, their training and inference often demand\n",
      "significant computational resources. For instance, models like BiomedGPT, Clinical-BERT, and Visual Med-Alpaca,\n",
      "with millions to billions of parameters, necessitate high-end GPUs or even dedicated hardware accelerators for efficient\n",
      "operation. It is essential to acknowledge that the hardware investment required for these models may present a challenge\n",
      "for resource-constrained healthcare institutions. Therefore, striking a balance between model performance and hardware\n",
      "feasibility is a crucial consideration when implementing these models in clinical practice. Future research should explore\n",
      "strategies to optimize these models for deployment on less resource-intensive hardware, making them accessible to a wider\n",
      "range of healthcare facilities.\n",
      "Dataset Sizes: Another noteworthy aspect is the size of the datasets used to train and fine-tune foundational models. Larger\n",
      "datasets often result in improved model performance and generalization, but they can be challenging to obtain in the medical\n",
      "domain due to privacy concerns and the labor-intensive nature of medical data annotation. Several papers in our survey\n",
      "have employed datasets with varying sizes, from thousands to millions of medical images and reports. Understanding the\n",
      "dataset size requirements for achieving state-of-the-art results is vital for healthcare practitioners and researchers. While\n",
      "some models demonstrate exceptional performance with relatively small datasets, others rely on extensive datasets to excel\n",
      "in complex medical tasks. Future research should explore techniques for efficient dataset collection, augmentation, and\n",
      "utilization, enabling the development of models that can perform well with limited data while preserving patient privacy.\n",
      "To provide a more detailed overview of the hardware requirements and dataset sizes reported in the reviewed papers, we\n",
      "present Table 2. In this table, we present sample hardware configurations utilized for training the network, alongside details\n",
      "regarding dataset sizes.\n",
      "5 Open challenges and Future Direction\n",
      "Throughout this survey, we have conducted an in-depth analysis of various foundational models, delving into their\n",
      "architectural designs, motivations, objectives, and use cases, all aimed at tackling real-world challenges. In this section, our\n",
      "focus shifts to underscore research directions that have the potential to further empower these models for addressing medical\n",
      "imaging applications.\n",
      "18Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision\n",
      "5.1 Open-source Multimodal Models\n",
      "The future direction of foundation models in medical imaging holds immense promise, primarily due to their seamless\n",
      "integration of diverse data modalities. This integration creates opportunities to explore medical concepts at multiple\n",
      "scales and leverage insights from various knowledge sources, including imaging, textual, and audio data. This multimodal\n",
      "integration empowers medical discoveries that are challenging to achieve with single-modality data alone, while also\n",
      "facilitating knowledge transfer across domains [ 23]. For example, current self-supervised learning methods are not\n",
      "universally generalizable and often need to be tailored and developed for each specific modality, highlighting the ongoing\n",
      "need for research and innovation in this area. Foundation models are poised to revolutionize healthcare by offering a holistic\n",
      "understanding of diseases and enabling more precise and data-driven medical interventions. However, to truly unlock the full\n",
      "potential of foundation models in this context, we must emphasize the need to consider inter-modality and cross-modality\n",
      "relationships more effectively. This involves developing methods that can effectively bridge the gap between different data\n",
      "modalities, allowing for better information fusion and more accurate predictions. By enhancing the ability to capture the\n",
      "intricate connections between different medical data, we can further increase the performance and utility of foundation\n",
      "models in medical imaging and healthcare. This interdisciplinary approach is critical for advancing our understanding of\n",
      "complex diseases and improving patient care.\n",
      "5.2 Interpretablity\n",
      "Understanding a model’s capabilities, reasoning, and mechanisms provides profound insights into its outputs. Explainability\n",
      "and interpretability are pivotal in adopting foundation models for building trustworthy AI-driven systems and ensuring their\n",
      "ethical and practical use in healthcare [90]. These capabilities are essential for transparency, accountability, and regulatory\n",
      "compliance. Specifically, understanding what a model can do, why it behaves in certain ways, and how it operates is\n",
      "particularly vital when dealing with foundation models. These complex models, powered by extensive data, possess the\n",
      "ability to perform unforeseen tasks in entirely novel ways [1]. In healthcare, explainability is critical for decisions regarding\n",
      "patient symptoms, clinical trials, and informed consent. Transparent AI reasoning helps resolve disagreements between AI\n",
      "systems and human experts, explaining the reasons behind the created decisions. However, most current foundation models\n",
      "lack built-in explainability, requiring future research. By connecting AI outputs with medical knowledge, models become\n",
      "more understandable, enabling users to grasp not only what the model predicts but why. This interdisciplinary approach,\n",
      "merging AI with domain expertise, advances disease understanding, elevates patient care, and promotes responsible AI use\n",
      "in healthcare.\n",
      "5.3 Bias and Variance in Foundational Models\n",
      "Within the domain of foundational models for medical imaging, two critical aspects demand ongoing scrutiny and investiga-\n",
      "tion: bias and variance [91].\n",
      "Bias: One of the foremost challenges facing foundational models is the presence of bias in both data and predictions. Just as\n",
      "in vision and language models, foundational models in medical imaging can inherit and amplify biases present in the training\n",
      "data. These biases might be related to race, ethnicity, gender, or socioeconomic factors, and they can manifest in the models’\n",
      "predictions and behaviors. For instance, a model might exhibit disparities in disease diagnosis or treatment recommendations\n",
      "for different demographic groups, potentially leading to unequal healthcare outcomes. Thus, addressing and mitigating\n",
      "biases in foundational models is of paramount importance to ensure fairness, inclusivity, and ethical deployment in the\n",
      "medical domain.\n",
      "Variance: Variance, on the other hand, pertains to the models’ sensitivity to fluctuations in the training data. In the context of\n",
      "medical imaging, variance can manifest as the models’ inability to generalize effectively across diverse patient populations\n",
      "or different healthcare settings. Models with high variance might perform exceptionally well on one dataset but poorly on\n",
      "another, hindering their reliability in real-world clinical applications. Therefore, strategies that enhance the robustness and\n",
      "generalization capabilities of foundational models are crucial for their widespread adoption and utility.\n",
      "5.4 Adversarial Attacks\n",
      "In the healthcare system, where the accuracy of medical decisions can have life-altering consequences, susceptibility\n",
      "to adversarial attacks [92] is a pressing concern of paramount importance. These attacks, which involve the deliberate\n",
      "manipulation of model inputs, can lead to not only erroneous but potentially harmful outputs, creating a perilous landscape for\n",
      "medical practitioners and patients alike. For instance, in the context of medical imaging, adversarial attacks could potentially\n",
      "result in misdiagnoses, causing patients to receive incorrect treatments or delay necessary interventions. Furthermore, the\n",
      "compromise of patient data privacy through adversarial tactics can lead to severe breaches of confidentiality, raising ethical\n",
      "and legal concerns.\n",
      "19Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision\n",
      "Additionally, the potential for the spread of false medical information, fueled by adversarial attacks, could have far-reaching\n",
      "consequences, undermining public trust in foundational models and healthcare systems. Therefore, addressing these\n",
      "vulnerabilities and developing robust defence mechanisms are not just academic endeavors but essential imperatives for\n",
      "ensuring the safety, reliability, and ethical use of foundational models in medical applications. The healthcare domain\n",
      "demands a proactive stance in fortifying foundational models against adversarial threats to safeguard the integrity and\n",
      "efficacy of clinical decision-making processes and the privacy of patient data.\n",
      "5.5 Down-stream Task Adaptation\n",
      "Foundation models offer powerful adaptability, including fine-tuning and prompting, making them versatile for healthcare\n",
      "and medical tasks. However, their extensive initial training demands substantial resources, adapting them efficiently\n",
      "for different tasks without losing learned knowledge is a critical challenge, and there is a need for research to reduce\n",
      "computational and memory requirements for quick adaptation, as current approaches often require careful hyperparameter\n",
      "selection that can impact generalization performance. Therefore, these challenges point to the need for more efficient\n",
      "foundation models in the future to enhance their general-purpose utility.\n",
      "5.6 Extensive Data and Computational Demands\n",
      "Foundation models, while powerful, come with substantial computational costs for development, training, and deployment.\n",
      "In specific cases, smaller models can achieve similar or better results at a lower cost. Training large-scale models is data and\n",
      "compute-intensive, and acquiring extensive labeled data can be expensive and time-consuming, especially for specialized\n",
      "domains or less-resourced languages. Inference with these models is also costly due to their many parameters. A summary\n",
      "of the computational budget and training costs of some of the reviewed models in this paper is provided in Table 2.\n",
      "These computational demands hinder their practicality in real-world applications, particularly those needing real-time\n",
      "inference or running on resource-constrained edge and mobile devices. For instance, visual prompt-based models like\n",
      "Segment Anything [10], while having robust image encoders, currently lack real-time processing speed, a crucial requirement\n",
      "for practical use. FastSAM [93], on the other hand, achieves comparable performance to the SAM method but at 50 times\n",
      "faster run-time speed by replacing the Transformer architecture with YOLOv8-seg [94], significantly expanding the utility\n",
      "of such models in real-world scenarios. Consequently, there’s potential to develop more efficient successors to address this\n",
      "issue, particularly in medical applications where running models on edge devices offer substantial advantages, especially in\n",
      "underserved areas.\n",
      "5.7 Prompt Engineering\n",
      "Prompt engineering is a critical aspect of foundational models in medical imaging, and its significance lies in its potential to\n",
      "bridge the gap between these models and radiologists, ultimately enhancing patient care [ 55]. In the context of medical\n",
      "image interpretation, effective communication between radiologists and AI models can lead to several noteworthy benefits.\n",
      "First and foremost, prompt engineering allows radiologists to have natural and interactive conversations with AI models.\n",
      "This capability is particularly valuable as it enables radiologists to seek clarifications, provide additional context, and ask\n",
      "follow-up questions, mirroring real-world clinical scenarios. For example, when reviewing a complex medical image, a\n",
      "radiologist may need to ask the AI model for further explanations about its findings, request alternative views, or explore\n",
      "differential diagnoses. Prompt engineering facilitates this conversational flow, making AI models more accessible and\n",
      "collaborative tools for radiologists. Moreover, the ability to converse with AI models through well-constructed prompts\n",
      "empowers radiologists with a more interactive and intuitive workflow. Instead of relying solely on fixed queries or predefined\n",
      "prompts, radiologists can tailor their interactions based on the specific nuances of each case. This adaptability allows for a\n",
      "more dynamic and personalized user experience, ultimately improving diagnostic accuracy and efficiency. Furthermore,\n",
      "prompt engineering contributes to the interpretability and transparency of AI models. Radiologists can gain insights into\n",
      "how the model arrives at its conclusions by crafting prompts that elicit detailed explanations. This transparency is crucial in\n",
      "a clinical context, where radiologists need to understand the reasoning behind the AI model’s recommendations and trust its\n",
      "diagnostic insights.\n",
      "5.8 Lack of Effective Benchmark to Monitor Progress\n",
      "While various benchmark datasets and evaluation metrics exist, they often fall short in comprehensively assessing model\n",
      "performance across diverse medical imaging tasks, modalities, and real-world clinical scenarios. Addressing this issue\n",
      "and establishing a robust benchmarking framework is crucial for several reasons. Firstly, a comprehensive benchmark\n",
      "can facilitate fair and standardized model evaluation, enabling researchers to assess the true strengths and weaknesses of\n",
      "different foundational models accurately. Currently, models may excel in specific datasets or tasks but struggle when applied\n",
      "20Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision\n",
      "to new, untested scenarios. An effective benchmark should encompass a wide spectrum of medical imaging challenges,\n",
      "including rare conditions and diverse patient populations, to provide a holistic assessment of model capabilities. Secondly,\n",
      "a well-structured benchmark can drive innovation by defining clear objectives and goals for the field. It can serve as\n",
      "a reference point for researchers and encourage the development of models that can address real-world clinical needs\n",
      "effectively. Moreover, it can incentivize the creation of models that are robust, interpretable, and adaptable to the dynamic\n",
      "nature of healthcare. Lastly, an effective benchmarking framework can aid in the deployment of foundational models in\n",
      "clinical practice. By thoroughly evaluating models’ performance and generalization across various clinical settings, it can\n",
      "assist healthcare providers in selecting the most suitable models for specific tasks and ensure that AI-assisted medical\n",
      "decision-making is reliable and safe.\n",
      "5.9 Enhancing Feature Representation from Frequency Perspective\n",
      "Given that the majority of foundational models employ ViT models as their backbone, it becomes crucial to assess these\n",
      "models from a frequency perspective to ensure their ability to capture and learn diverse frequency information necessary for\n",
      "object recognition. Recent research has shed light on the fact that traditional self-attention mechanisms in ViT, while effective\n",
      "in mitigating local feature disparities, tend to neglect vital high-frequency details, such as textures and edge characteristics\n",
      "[95, 96]. This oversight is particularly problematic in tasks like tumor detection, cancer-type identification through radiomics\n",
      "analysis, and treatment response assessment, as these tasks often hinge on recognizing subtle textural abnormalities.\n",
      "Additionally, it’s worth noting that self-attention mechanisms come with a quadratic computational complexity and may\n",
      "generate redundant features [97]. Given these considerations, the design of new foundational models should take these\n",
      "limitations into account and explore potential enhancements. This could involve incorporating CNN layers or adopting more\n",
      "efficient ViT architectures to strike a balance between computational efficiency and preserving high-frequency information.\n",
      "6 Conclusion\n",
      "In this comprehensive survey, we have conducted an in-depth review of recent advancements in foundational models for\n",
      "medical imaging. Our survey commences with an introductory section that provides insight into the evolution of foundation\n",
      "models and their potential contributions to the healthcare sector.\n",
      "Subsequently, we categorize these models into four main groups, differentiating between those prompted by text and those\n",
      "guided by visual cues. Each of these categories boasts unique strengths and capabilities, and in Section 3, we delve into\n",
      "these directions by presenting exemplary works and offering comprehensive methodological descriptions. Furthermore, our\n",
      "exploration extends to evaluating the advantages and limitations inherent to each model type. We shed light on their areas of\n",
      "excellence and identify areas where they have room for improvement. This information is presented in the form of pros,\n",
      "cons, and real-world use cases of these models in the context of medical imaging scenarios, and the summarized results can\n",
      "be found in Table 1. Moreover, we consider the hardware and dataset requirements for implementing these models. We\n",
      "provide various configuration strategies to elucidate the prerequisites for future research endeavors, helping researchers gain\n",
      "a clear understanding of the necessary resources. In conclusion, our survey not only reviews recent developments but also\n",
      "sets the stage for future research in foundational models. We propose several directions for future investigations, offering a\n",
      "roadmap for researchers to excel in the field of foundational models for medical imaging.\n",
      "References\n",
      "[1] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein,\n",
      "Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv\n",
      "preprint arXiv:2108.07258, 2021.\n",
      "[2] Muhammad Awais, Muzammal Naseer, Salman Khan, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah,\n",
      "Ming-Hsuan Yang, and Fahad Shahbaz Khan. Foundational models defining a new era in vision: A survey and outlook.\n",
      "arXiv preprint arXiv:2307.13721, 2023.\n",
      "[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\n",
      "Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural\n",
      "information processing systems, 33:1877–1901, 2020.\n",
      "[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,\n",
      "Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways.\n",
      "arXiv preprint arXiv:2204.02311, 2022.\n",
      "21Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision\n",
      "[5] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton,\n",
      "Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085,\n",
      "2022.\n",
      "[6] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste\n",
      "Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models.\n",
      "arXiv preprint arXiv:2302.13971, 2023.\n",
      "[7] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and\n",
      "Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. InInternational\n",
      "conference on machine learning, pages 4904–4916. PMLR, 2021.\n",
      "[8] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and\n",
      "Chunjing Xu. Filip: Fine-grained interactive language-image pre-training. arXiv preprint arXiv:2111.07783, 2021.\n",
      "[9] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\n",
      "Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.\n",
      "arXiv preprint arXiv:2111.02114, 2021.\n",
      "[10] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer\n",
      "Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023.\n",
      "[11] Junlong Cheng, Jin Ye, Zhongying Deng, Jianpin Chen, Tianbin Li, Haoyu Wang, Yanzhou Su, Ziyan Huang, Jilong\n",
      "Chen, Lei Jiang, Hui Sun, Junjun He, Shaoting Zhang, Min Zhu, and Yu Qiao. Sam-med2d, 2023.\n",
      "[12] Jun Ma and Bo Wang. Segment anything in medical images. arXiv preprint arXiv:2304.12306, 2023.\n",
      "[13] Wenhui Lei, Xu Wei, Xiaofan Zhang, Kang Li, and Shaoting Zhang. Medlsam: Localize and segment anything model\n",
      "for 3d medical images. arXiv preprint arXiv:2306.14752, 2023.\n",
      "[14] Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin Jin, Wenjun Zeng, and Zhibo Chen. Inpaint anything: Segment\n",
      "anything meets image inpainting. arXiv preprint arXiv:2304.06790, 2023.\n",
      "[15] Songhua Liu, Jingwen Ye, and Xinchao Wang. Any-to-any style transfer: Making picasso and da vinci collaborate.\n",
      "arXiv e-prints, pages arXiv–2304, 2023.\n",
      "[16] Teng Wang, Jinrui Zhang, Junjie Fei, Yixiao Ge, Hao Zheng, Yunlong Tang, Zhe Li, Mingqi Gao, Shanshan Zhao,\n",
      "Ying Shan, et al. Caption anything: Interactive image description with diverse multimodal controls. arXiv preprint\n",
      "arXiv:2305.02677, 2023.\n",
      "[17] Zhanghexuan Ji, Dazhou Guo, Puyang Wang, Ke Yan, Jia Ge, Xianghua Ye, Minfeng Xu, Jingren Zhou, Le Lu,\n",
      "Mingchen Gao, et al. Continual segment: Towards a single, unified and accessible continual segmentation model of\n",
      "143 whole-body organs in ct scans. arXiv preprint arXiv:2302.00162, 2023.\n",
      "[18] Yunkun Zhang, Jin Gao, Mu Zhou, Xiaosong Wang, Yu Qiao, Shaoting Zhang, and Dequan Wang. Text-guided\n",
      "foundation model adaptation for pathological image classification. arXiv preprint arXiv:2307.14901, 2023.\n",
      "[19] Reza Azad, Amirhossein Kazerouni, Moein Heidari, Ehsan Khodapanah Aghdam, Amirali Molaei, Yiwei Jia, Abin\n",
      "Jose, Rijo Roy, and Dorit Merhof. Advances in medical image analysis with vision transformers: A comprehensive\n",
      "review. Medical Image Analysis, 2023.\n",
      "[20] Zhao Wang, Chang Liu, Shaoting Zhang, and Qi Dou. Foundation model for endoscopy video analysis via large-scale\n",
      "self-supervised pre-train. arXiv preprint arXiv:2306.16741, 2023.\n",
      "[21] Duy MH Nguyen, Hoang Nguyen, Nghiem T Diep, Tan N Pham, Tri Cao, Binh T Nguyen, Paul Swoboda, Nhat Ho,\n",
      "Shadi Albarqouni, Pengtao Xie, et al. Lvm-med: Learning large-scale self-supervised vision models for medical\n",
      "imaging via second-order graph matching. arXiv preprint arXiv:2306.11925, 2023.\n",
      "[22] Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew Carroll,\n",
      "Chuck Lau, Ryutaro Tanno, Ira Ktena, Basil Mustafa, Aakanksha Chowdhery, Yun Liu, Simon Kornblith, David Fleet,\n",
      "Philip Mansfield, Sushant Prakash, Renee Wong, Sunny Virmani, Christopher Semturs, S Sara Mahdavi, Bradley\n",
      "Green, Ewa Dominowska, Blaise Aguera y Arcas, Joelle Barral, Dale Webster, Greg S. Corrado, Yossi Matias, Karan\n",
      "Singhal, Pete Florence, Alan Karthikesalingam, and Vivek Natarajan. Towards generalist biomedical ai, 2023.\n",
      "[23] Michael Moor, Oishi Banerjee, Zahra Shakeri Hossein Abad, Harlan M Krumholz, Jure Leskovec, Eric J Topol, and\n",
      "Pranav Rajpurkar. Foundation models for generalist medical artificial intelligence. Nature, 616(7956):259–265, 2023.\n",
      "[24] Peilun Shi, Jianing Qiu, Sai Mu Dalike Abaxi, Hao Wei, Frank P-W Lo, and Wu Yuan. Generalist vision foundation\n",
      "models for medical imaging: A case study of segment anything model on zero-shot medical segmentation. Diagnostics,\n",
      "13(11):1947, 2023.\n",
      "22Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision\n",
      "[25] Kai Zhang, Jun Yu, Zhiling Yan, Yixin Liu, Eashan Adhikarla, Sunyang Fu, Xun Chen, Chen Chen, Yuyin Zhou,\n",
      "Xiang Li, Lifang He, Brian D. Davison, Quanzheng Li, Yong Chen, Hongfang Liu, and Lichao Sun. Biomedgpt: A\n",
      "unified and generalist biomedical generative pre-trained transformer for vision, language, and multimodal tasks, 2023.\n",
      "[26] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Towards generalist foundation model for\n",
      "radiology, 2023.\n",
      "[27] Yukun Zhou, Mark A Chia, Siegfried K Wagner, Murat S Ayhan, Dominic J Williamson, Robbert R Struyven, Timing\n",
      "Liu, Moucheng Xu, Mateo G Lozano, Peter Woodward-Court, et al. A foundation model for generalizable disease\n",
      "detection from retinal images. Nature, pages 1–8, 2023.\n",
      "[28] Qiuhui Chen, Xinyue Hu, Zirui Wang, and Yi Hong. Medblip: Bootstrapping language-image pre-training from 3d\n",
      "medical images and texts. arXiv preprint arXiv:2305.10799, 2023.\n",
      "[29] Yakoub Bazi, Mohamad Mahmoud Al Rahhal, Laila Bashmal, and Mansour Zuair. Vision&ndash;language model for\n",
      "visual question answering in medical imagery. Bioengineering, 10(3), 2023.\n",
      "[30] Ekin Tiu, Ellie Talius, Pujan Patel, Curtis P Langlotz, Andrew Y Ng, and Pranav Rajpurkar. Expert-level detection\n",
      "of pathologies from unannotated chest x-ray images via self-supervised learning. Nature Biomedical Engineering,\n",
      "6(12):1399–1406, 2022.\n",
      "[31] Zifeng Wang, Zhenbang Wu, Dinesh Agarwal, and Jimeng Sun. Medclip: Contrastive learning from unpaired medical\n",
      "images and text. arXiv preprint arXiv:2210.10163, 2022.\n",
      "[32] Shruthi Bannur, Stephanie Hyland, Qianchu Liu, Fernando Perez-Garcia, Maximilian Ilse, Daniel C Castro, Benedikt\n",
      "Boecking, Harshita Sharma, Kenza Bouzid, Anja Thieme, et al. Learning to exploit temporal structure for biomedical\n",
      "vision-language processing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n",
      "pages 15016–15027, 2023.\n",
      "[33] Jie Liu, Yixiao Zhang, Jie-Neng Chen, Junfei Xiao, Yongyi Lu, Bennett A Landman, Yixuan Yuan, Alan Yuille,\n",
      "Yucheng Tang, and Zongwei Zhou. Clip-driven universal model for organ segmentation and tumor detection.arXiv\n",
      "preprint arXiv:2301.00785, 2023.\n",
      "[34] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao, Mu Wei, Naveen\n",
      "Valluri, Cliff Wong, et al. Large-scale domain-specific pretraining for biomedical vision-language processing. arXiv\n",
      "preprint arXiv:2303.00915, 2023.\n",
      "[35] Zhihong Chen, Shizhe Diao, Benyou Wang, Guanbin Li, and Xiang Wan. Towards unifying medical vision-and-\n",
      "language pre-training via soft prompts. arXiv preprint arXiv:2302.08958, 2023.\n",
      "[36] Ming Y Lu, Bowen Chen, Andrew Zhang, Drew FK Williamson, Richard J Chen, Tong Ding, Long Phi Le, Yung-Sung\n",
      "Chuang, and Faisal Mahmood. Visual language pretrained multiple instance zero-shot transfer for histopathology\n",
      "images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19764–19775,\n",
      "2023.\n",
      "[37] Xiaofei Chen, Yuting He, Cheng Xue, Rongjun Ge, Shuo Li, and Guanyu Yang. Knowledge boosting: Rethinking\n",
      "medical contrastive vision-language pre-training. arXiv preprint arXiv:2307.07246, 2023.\n",
      "[38] Zhi Huang, Federico Bianchi, Mert Yuksekgonul, Thomas J Montine, and James Zou. A visual–language foundation\n",
      "model for pathology image analysis using medical twitter. Nature Medicine, pages 1–10, 2023.\n",
      "[39] Shawn Xu, L. Yang, Christopher J. Kelly, Marcin Sieniek, Timo Kohlberger, Martin Q. Ma, Wei-Hung Weng,\n",
      "Attila Péter Király, Sahar Kazemzadeh, Zakkai Melamed, Jungyeon Park, Patricia Strachan, Yun Liu, Charles Lau,\n",
      "Preeti Singh, Christina Chen, Mozziyar Etemadi, Sreenivasa Raju Kalidindi, Yossi Matias, Katherine Chou, Greg S\n",
      "Corrado, Shravya Shetty, Daniel Tse, Shruthi Prabhakara, Daniel Golden, Rory Pilgrim, Krish Eswaran, and Andrew\n",
      "Sellergren. Elixr: Towards a general purpose x-ray artificial intelligence system through alignment of large language\n",
      "models and radiology vision encoders. ArXiv, abs/2308.01317, 2023.\n",
      "[40] Weijian Huang, Hongyu Zhou, Cheng Li, Hao Yang, Jiarun Liu, and Shanshan Wang. Enhancing representation in\n",
      "radiography-reports foundation model: A granular alignment algorithm using masked contrastive learning. arXiv\n",
      "preprint arXiv:2309.05904, 2023.\n",
      "[41] Bin Yan and Mingtao Pei. Clinical-bert: Vision-language pre-training for radiograph diagnosis and reports generation.\n",
      "In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 2982–2990, 2022.\n",
      "[42] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather\n",
      "Cole-Lewis, Darlene Neal, et al. Towards expert-level medical question answering with large language models. arXiv\n",
      "preprint arXiv:2305.09617, 2023.\n",
      "[43] Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Cyril Zakka, Yash Dalmia, Eduardo Pontes Reis, Pranav\n",
      "Rajpurkar, and Jure Leskovec. Med-flamingo: a multimodal medical few-shot learner.arXiv preprint arXiv:2307.15189,\n",
      "2023.\n",
      "23Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision\n",
      "[44] Guoyao Deng, Ke Zou, Kai Ren, Meng Wang, Xuedong Yuan, Sancong Ying, and Huazhu Fu. Sam-u: Multi-box\n",
      "prompts triggered uncertainty estimation for reliable sam in medical image, 2023.\n",
      "[45] Xinrong Hu, Xiaowei Xu, and Yiyu Shi. How to efficiently adapt large segmentation model (sam) to medical images.\n",
      "arXiv preprint arXiv:2306.13731, 2023.\n",
      "[46] Junde Wu, Rao Fu, Huihui Fang, Yuanpei Liu, Zhaowei Wang, Yanwu Xu, Yueming Jin, and Tal Arbel. Medical sam\n",
      "adapter: Adapting segment anything model for medical image segmentation. arXiv preprint arXiv:2304.12620, 2023.\n",
      "[47] Kaidong Zhang and Dong Liu. Customized segment anything model for medical image segmentation. arXiv preprint\n",
      "arXiv:2304.13785, 2023.\n",
      "[48] Eugene V orontsov, Alican Bozkurt, Adam Casson, George Shaikovski, Michal Zelechowski, Siqi Liu, Philippe\n",
      "Mathieu, Alexander van Eck, Donghun Lee, Julian Viret, et al. Virchow: A million-slide digital pathology foundation\n",
      "model. arXiv preprint arXiv:2309.07778, 2023.\n",
      "[49] Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-llama: Towards building\n",
      "open-source language models for medicine, 2023.\n",
      "[50] Guangyu Wang, Guoxing Yang, Zongxin Du, Longjun Fan, and Xiaohu Li. Clinicalgpt: Large language models\n",
      "finetuned with diverse medical data and comprehensive evaluation, 2023.\n",
      "[51] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung\n",
      "Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision assistant for biomedicine in one day. arXiv\n",
      "preprint arXiv:2306.00890, 2023.\n",
      "[52] Omkar Thawkar, Abdelrahman Shaker, Sahal Shaji Mullappilly, Hisham Cholakkal, Rao Muhammad Anwer, Salman\n",
      "Khan, Jorma Laaksonen, and Fahad Shahbaz Khan. Xraygpt: Chest radiographs summarization using medical\n",
      "vision-language models. arXiv preprint arXiv:2306.07971, 2023.\n",
      "[53] Zhengliang Liu, Yiwei Li, Peng Shu, Aoxiao Zhong, Longtao Yang, Chao Ju, Zihao Wu, Chong Ma, Jie Luo, Cheng\n",
      "Chen, Sekeun Kim, Jiang Hu, Haixing Dai, Lin Zhao, Dajiang Zhu, Jun Liu, Wei Liu, Dinggang Shen, Tianming Liu,\n",
      "Quanzheng Li, and Xiang Li. Radiology-llama2: Best-in-class large language model for radiology, 2023.\n",
      "[54] Sheng Wang, Zihao Zhao, Xi Ouyang, Qian Wang, and Dinggang Shen. Chatcad: Interactive computer-aided diagnosis\n",
      "on medical image using large language models. arXiv preprint arXiv:2302.07257, 2023.\n",
      "[55] Zheng-Long Liu, Xiao-Xing Yu, Lu Zhang, Zihao Wu, Chao-Yang Cao, Haixing Dai, Lin Zhao, W. Liu, Dinggang\n",
      "Shen, Quanzheng Li, Tianming Liu, Dajiang Zhu, and Xiang Li. Deid-gpt: Zero-shot medical text de-identification by\n",
      "gpt-4. ArXiv, abs/2303.11032, 2023.\n",
      "[56] Li Yunxiang, Li Zihan, Zhang Kai, Dan Ruilong, and Zhang You. Chatdoctor: A medical chat model fine-tuned on\n",
      "llama model using medical domain knowledge. arXiv preprint arXiv:2303.14070, 2023.\n",
      "[57] Chang Shu, Chen Baian, Fangyu Liu, Zihao Fu, Ehsan Shareghi, and Nigel Collier. Visual med-alpaca: A parameter-\n",
      "efficient biomedical llm with visual capabilities. https://cambridgeltl.github.io/visual-med-alpaca/.\n",
      "Accessed: 2023-09-01.\n",
      "[58] Amirhossein Kazerouni, Ehsan Khodapanah Aghdam, Moein Heidari, Reza Azad, Mohsen Fayyaz, Ilker Hacihaliloglu,\n",
      "and Dorit Merhof. Diffusion models in medical imaging: A comprehensive survey. Medical Image Analysis, page\n",
      "102846, 2023.\n",
      "[59] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie\n",
      "Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.\n",
      "[60] Lizhou Fan, Lingyao Li, Zihui Ma, Sanggyu Lee, Huizi Yu, and Libby Hemphill. A bibliometric review of large\n",
      "language models research from 2017 to 2023. arXiv preprint arXiv:2304.02020, 2023.\n",
      "[61] Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey. arXiv preprint\n",
      "arXiv:2212.10403, 2022.\n",
      "[62] Jindong Gu, Zhen Han, Shuo Chen, Ahmad Beirami, Bailan He, Gengyuan Zhang, Ruotong Liao, Yao Qin, V olker\n",
      "Tresp, and Philip Torr. A systematic survey of prompt engineering on vision-language foundation models. arXiv\n",
      "preprint arXiv:2307.12980, 2023.\n",
      "[63] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. Vision-language models for vision tasks: A survey. arXiv\n",
      "preprint arXiv:2304.00685, 2023.\n",
      "[64] Shaoting Zhang and Dimitris Metaxas. On the challenges and perspectives of foundation models for medical image\n",
      "analysis. arXiv preprint arXiv:2306.05705, 2023.\n",
      "[65] Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie. Slip: Self-supervision meets language-image\n",
      "pre-training, 2021.\n",
      "24Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision\n",
      "[66] J. Yang, C. Li, P. Zhang, B. Xiao, C. Liu, L. Yuan, and J. Gao. Unified contrastive learning in image-text-label space.\n",
      "In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 19141–19151, 2022.\n",
      "[67] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual\n",
      "representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition ,\n",
      "pages 9729–9738, 2020.\n",
      "[68] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda\n",
      "Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In\n",
      "International conference on machine learning, pages 8748–8763. PMLR, 2021.\n",
      "[69] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning\n",
      "of visual representations. In International conference on machine learning, pages 1597–1607. PMLR, 2020.\n",
      "[70] Aäron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. ArXiv,\n",
      "abs/1807.03748, 2018.\n",
      "[71] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E. Hinton. Big self-supervised\n",
      "models are strong semi-supervised learners. In NeurIPS, 2020.\n",
      "[72] Lewei Yao, Runhu Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and\n",
      "Chunjing Xu. Filip: Fine-grained interactive language-image pre-training. ArXiv, abs/2111.07783, 2021.\n",
      "[73] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang,\n",
      "Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In Proceedings of the\n",
      "IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10965–10975, 2022.\n",
      "[74] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Li, Xiyang Dai, Lijuan Wang, Lu Yuan,\n",
      "Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: Unifying localization and vision-language understanding. Advances in\n",
      "Neural Information Processing Systems, 35:36067–36080, 2022.\n",
      "[75] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. ArXiv, abs/2106.08254, 2021.\n",
      "[76] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\n",
      "Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv, abs/1907.11692,\n",
      "2019.\n",
      "[77] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach,\n",
      "and Douwe Kiela. Flava: A foundational language and vision alignment model. In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition, pages 15638–15650, 2022.\n",
      "[78] Xinsong Zhang, Yan Zeng, Jipeng Zhang, and Hang Li. Toward building general foundation models for language,\n",
      "vision, and vision-language understanding tasks. arXiv preprint arXiv:2301.05065, 2023.\n",
      "[79] Michael Tschannen, Manoj Kumar, Andreas Steiner, Xiaohua Zhai, Neil Houlsby, and Lucas Beyer. Image captioners\n",
      "are scalable vision learners too. arXiv preprint arXiv:2306.07915, 2023.\n",
      "[80] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A generalist\n",
      "painter for in-context visual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition, pages 6830–6839, 2023.\n",
      "[81] Huaishao Luo, Junwei Bao, Youzheng Wu, Xiaodong He, and Tianrui Li. Segclip: Patch aggregation with learnable\n",
      "centers for open-vocabulary semantic segmentation. In International Conference on Machine Learning , pages\n",
      "23033–23044. PMLR, 2023.\n",
      "[82] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\n",
      "Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback.\n",
      "Advances in Neural Information Processing Systems, 35:27730–27744, 2022.\n",
      "[83] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna\n",
      "Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv\n",
      "preprint arXiv:2212.08073, 2022.\n",
      "[84] Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq R. Joty, Caiming Xiong, and Steven C. H. Hoi.\n",
      "Align before fuse: Vision and language representation learning with momentum distillation. In Neural Information\n",
      "Processing Systems, 2021.\n",
      "[85] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. Blip-2: Bootstrapping language-image pre-training with\n",
      "frozen image encoders and large language models. ArXiv, abs/2301.12597, 2023.\n",
      "[86] A Venigalla, J Frankle, and M Carbin. Biomedlm: a domain-specific large language model for biomedical text.\n",
      "MosaicML. Accessed: Dec, 23, 2022.\n",
      "25Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision\n",
      "[87] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\n",
      "Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.\n",
      "[88] Shizhan Gong, Yuan Zhong, Wenao Ma, Jinpeng Li, Zhao Wang, Jingyang Zhang, Pheng-Ann Heng, and Qi Dou.\n",
      "3dsam-adapter: Holistic adaptation of sam from 2d to 3d for promptable medical image segmentation. arXiv preprint\n",
      "arXiv:2306.13465, 2023.\n",
      "[89] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-llama: Further finetuning llama on\n",
      "medical papers. arXiv preprint arXiv:2304.14454, 2023.\n",
      "[90] Reza Azad, Ehsan Khodapanah Aghdam, Amelie Rauland, Yiwei Jia, Atlas Haddadi Avval, Afshin Bozorgpour, Sanaz\n",
      "Karimijafarbigloo, Joseph Paul Cohen, Ehsan Adeli, and Dorit Merhof. Medical image segmentation review: The\n",
      "success of u-net. arXiv preprint arXiv:2211.14830, 2022.\n",
      "[91] Zitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, and Yi Ma. Rethinking bias-variance trade-off for gen-\n",
      "eralization of neural networks. In International Conference on Machine Learning , pages 10767–10777. PMLR,\n",
      "2020.\n",
      "[92] Natalie Maus, Patrick Chao, Eric Wong, and Jacob R Gardner. Black box adversarial prompting for foundation models.\n",
      "In The Second Workshop on New Frontiers in Adversarial Machine Learning, 2023.\n",
      "[93] Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, and Jinqiao Wang. Fast segment\n",
      "anything. arXiv preprint arXiv:2306.12156, 2023.\n",
      "[94] Glenn Jocher, Ayush Chaurasia, and Jing Qiu. YOLO by Ultralytics, January 2023.\n",
      "[95] Peihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang. Anti-oversmoothing in deep vision transformers\n",
      "via the fourier domain analysis: From theory to practice. In International Conference on Learning Representations,\n",
      "2022.\n",
      "[96] Reza Azad, Amirhossein Kazerouni, Babak Azad, Ehsan Khodapanah Aghdam, Yury Velichko, Ulas Bagci, and\n",
      "Dorit Merhof. Laplacian-former: Overcoming the limitations of vision transformers in local texture detection. In\n",
      "International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 736–746. Springer,\n",
      "2023.\n",
      "[97] Reza Azad, Leon Niggemeier, Michael Huttemann, Amirhossein Kazerouni, Ehsan Khodapanah Aghdam, Yury\n",
      "Velichko, Ulas Bagci, and Dorit Merhof. Beyond self-attention: Deformable large kernel attention for medical image\n",
      "segmentation. arXiv preprint arXiv:2309.00121, 2023.\n",
      "26' metadata={'source': 'sample_paper_raw_text.txt'}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f35940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dc510e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cab5c2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='1.4 Paper Organization.\n",
      "The rest of the survey is organized as follows. Section 2 presents the background and preliminaries for foundation models.\n",
      "We adopt the taxonomy of [2] and categorize previous studies into two main groups: those prompted by textual inputs\n",
      "(discussed in section 3.1) and those driven by visual cues (discussed in section 3.2). In the context of textually prompted' metadata={'source': 'sample_paper_raw_text.txt'}\n"
     ]
    }
   ],
   "source": [
    "print(chunks[57])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "789367f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93a8d25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e9d8702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c109f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"rag_local_collection\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e0e6fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc3d676f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0ffc97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"gemma3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3d55d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "02ca6a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a RAG assistant. Answer the question based solely on the context provided below. If the answer is not in context, please state that you don\\'t know.\n",
    "\n",
    "    Contexto: {context}\n",
    "\n",
    "    Pergunta: {input}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97aae9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8f2621bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c095bd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "def63a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4f003a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'According to the abstract, what primary gap are Foundation Models (FMs) trained to bridge, and what capabilities do they facilitate at test time?'\n",
    "response = rag_chain.invoke({\"input\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "588c9545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: According to the abstract, Foundation Models (FMs) are trained to bridge the gap between different modalities. They facilitate contextual reasoning, generalization, and prompt capabilities at test time.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Answer: {response['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "701d4477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'According to the abstract, what primary gap are Foundation Models (FMs) trained to bridge, and what capabilities do they facilitate at test time?',\n",
       " 'context': [Document(metadata={'source': 'sample_paper_raw_text.txt'}, page_content='development of Foundation Models (FMs). Foundation Models (FMs) are a type of artificial intelligence (AI) model that\\nexhibit significant progress in their development. These models are typically trained on extensive, diverse dataset, frequently\\nutilizing self-supervision techniques on a massive scale. Following this initial training, they can be further adapted, such as\\nthrough fine-tuning, for a wide array of downstream tasks that are related to the original training data [1].'),\n",
       "  Document(metadata={'source': 'sample_paper_raw_text.txt'}, page_content='tasks have gained significant interest lately in various deep-learning problems undergoing a paradigm\\nshift with the rise of these models. Trained on large-scale dataset to bridge the gap between different\\nmodalities, foundation models facilitate contextual reasoning, generalization, and prompt capabilities\\nat test time. The predictions of these models can be adjusted for new tasks by augmenting the model'),\n",
       "  Document(metadata={'source': 'sample_paper_raw_text.txt'}, page_content='foundation models, we further subdivide them into contrastive, generative, hybrid (combining contrastive and generative\\napproaches), and conversational visual language models. In addition, we differentiate textually prompted models into\\nadaptations and generalist models. Furthermore, Section 5 reveals the risk, open problems, and future directions of\\nfoundation models. Finally, we conclude our research in Section 6.'),\n",
       "  Document(metadata={'source': 'sample_paper_raw_text.txt'}, page_content='we initiate our exploration by providing an exposition of the fundamental concepts forming the basis\\nof foundation models. Subsequently, we offer a methodical taxonomy of foundation models within the\\nmedical domain, proposing a classification system primarily structured around training strategies, while\\nalso incorporating additional facets such as application domains, imaging modalities, specific organs of')],\n",
       " 'answer': 'According to the abstract, Foundation Models (FMs) are trained to bridge the gap between different modalities. They facilitate contextual reasoning, generalization, and prompt capabilities at test time.'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7316ed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question: str):\n",
    "    response = rag_chain.invoke({\"input\": question})\n",
    "    print(f\"Answer: {response['answer']}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7bedaec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Based on the context, foundation models offer an efficiency advantage concerning data usage for downstream tasks by being able to be adjusted for new tasks by augmenting the model databases.\n"
     ]
    }
   ],
   "source": [
    "ask('In contrast to the conventional deep learning paradigm, what is the main efficiency advantage that Foundation Models offer concerning data usage for downstream tasks?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "24af7467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The survey proposes a methodical taxonomy of foundation models within the medical domain, proposing a classification system primarily structured around training strategies, while also incorporating additional facets such as application domains, imaging modalities, specific organs of concern. The two main, broad categories used to classify existing works are those prompted by text and those guided by visual cues.\n"
     ]
    }
   ],
   "source": [
    "ask('The survey proposes a methodical taxonomy of Foundation Models in medical imaging. What are the two main, broad categories used to classify existing works, based on the type of input prompt they primarily use?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "602a30c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The paper mentions that foundation models help address the significant, domain-specific challenge of “intricate connections between different medical data” by allowing knowledge transfer without direct data access.\n"
     ]
    }
   ],
   "source": [
    "ask('Beyond the common challenges of interpretability and computational requirements, which significant, domain-specific challenge in medical imaging does the paper mention that Foundation Models help address by allowing knowledge transfer without direct data access?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
